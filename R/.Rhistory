stop("gb.tree.increase.set must be an integer-valued scalar or vector.")
} else if (length(gb.tree.increase.set) > 1 &
length(gb.tree.increase.set) != length(gb.shrinkage.set)) {
stop(paste("gb.tree.increase.set must be either a scalar or a vector of ",
"size `length(gb.shrinkage.set)`.", sep = ""))
}
if (!(is.integer(gb.trees.max.set) |
all(as.integer(gb.trees.max.set) == gb.trees.max.set))) {
stop("gb.trees.max.set must be an integer-valued scalar or vector.")
} else if (length(gb.trees.max.set) > 1 &
length(gb.trees.max.set) != length(gb.shrinkage.set)) {
stop(paste("gb.trees.max.set must be either a scalar or a vector of size ",
"`length(gb.shrinkage.set)`.", sep = ""))
}
# ------------------------------- Prepare data -------------------------------
# If not provided in census data, calculate bin size for each ideal type in
# a geographic unit
if (is.null(bin.size)) {
census <- census %>%
dplyr::group_by(.dots = c(L1.x, L2.unit)) %>%
dplyr::summarise(n = dplyr::n())
} else {
census$n <- census[[bin.size]]
}
# In census data, calculate bin proportion for each ideal type in a
# geographic unit
census <- census %>%
dplyr::group_by(.dots = L2.unit) %>%
dplyr::mutate(prop = n / sum(n))
# Scale context-level variables in survey and census data
survey[, L2.x] <- scale(survey[, L2.x], center = TRUE, scale = TRUE)
census[, L2.x] <- scale(census[, L2.x], center = TRUE, scale = TRUE)
# Compute principal components for survey data
pca_out <- stats::prcomp(survey[, L2.x],
retx = TRUE,
center = TRUE,
scale. = TRUE,
tol = NULL)
# Add PCs to survey data
survey <- survey %>%
dplyr::bind_cols(as.data.frame(pca_out$x))
# Add PCs to census data
pc_names <- colnames(pca_out$x)
census <- census %>%
dplyr::left_join(unique(dplyr::select(survey, L2.unit, pc_names)),
by = L2.unit)
# ------------------------------- Create folds -------------------------------
# EBMA hold-out fold
ebma_folding_out <- ebma_folding(data = survey,
L2.unit = L2.unit,
ebma.size = ebma.size)
ebma_fold <- ebma_folding_out$ebma_fold
cv_data <- ebma_folding_out$cv_data
# K folds for cross-validation
cv_folds <- cv_folding(data = cv_data,
L2.unit = L2.unit,
k.folds = k.folds,
cv.sampling = cv.sampling)
# ------------------------ Run individual classifiers ------------------------
# Classifier 1: Best Subset
best_subset_out <- best_subset(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.measure = loss.measure,
data = cv_folds,
verbose = verbose)
# Classifier 2: Lasso
lasso_out <- lasso(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.measure = loss.measure,
lambda.set = lasso.lambda.set,
iterations.max = lasso.iterations.max,
data = cv_folds,
verbose = verbose)
# Classifier 3: PCA
pca_out <- pca(y = y,
L1.x = L1.x,
L2.x = pc_names,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.measure = loss.measure,
data = cv_folds,
verbose = verbose)
# Classifier 4: GB
gb_out <- gb(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
L2.unit.include = gb.L2.unit.include,
L2.reg.include = gb.L2.reg.include,
loss.unit = loss.unit,
loss.measure = loss.measure,
interaction.set = gb.interaction.set,
shrinkage.set = gb.shrinkage.set,
tree.start = gb.tree.start,
tree.increase.set = gb.tree.increase.set,
trees.max.set = gb.trees.max.set,
iterations.max = gb.iterations.max,
n.minobsinnode = gb.n.minobsinnode,
data = cv_folds,
verbose = verbose)
# Classifier 5: SVM
svm_out <- svm(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
kernel = svm.kernel,
error.fun = svm.error.fun,
gamma.set = svm.gamma.set,
cost.set = svm.cost.set,
k.folds = k.folds,
data = cv_folds,
verbose = verbose)
# --------------------------- Post-stratification ----------------------------
ps_out <- post_stratification(data = cv_folds,
census = census,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
best.subset = best_subset_out,
pca = pca_out,
lasso = lasso_out,
gb = gb_out,
n.minobsinnode = gb.n.minobsinnode,
L2.unit.include = gb.L2.unit.include,
L2.reg.include = gb.L2.reg.include,
svm.out = svm_out,
kernel = svm.kernel,
verbose = verbose)
# ----------------------------------- EBMA ------------------------------------
ebma.out <- ebma(ebma.fold = ebma_fold,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
post.strat = ps_out,
Ndraws = Ndraws,
tol.values = tol.values,
best.subset = best_subset_out,
pca = pca_out,
lasso = lasso_out,
gb = gb_out,
svm.out = svm_out,
verbose = verbose)
ebma.out
print(ebma.out, n = nrow(ebma.out))
# This file contains code used for testing
# File should be removed after finishing package
# Load packages
library(magrittr)
# Clean working environment
rm(list = ls())
# Load survey data
load(here::here("data", "survey_sample.RData"))
# Load census data
load(here::here("data", "census_data.RData"))
# Assign data sets to objects with "correct" names
survey <- survey_sample
census <- census_data
# Define individual-level covariates
L1.x <- c("age", "educ", "gXr")
# Define context-level covariates
L2.x <- c("pvote", "religcon", "urban", "unemp", "hispanics", "white")
# Define context-level unit
L2.unit <- "stateid"
# Define region in which context-level units are nested
L2.reg <- "region"
# Define outcome variable
y <- "y"
# Define column in census containing bin size
bin.size <- "n"
# Define fraction of EBMA size (NULL defaults to 1/3)
ebma.size <- NULL
# Define number of folds
k.folds <- 5
# Unit to be used in sampling to create CV folds
cv.sampling <- "L2 units"
# Set seed (NULL defaults to 12345)
seed <- NULL
# Set verbose option
verbose <- TRUE
# Define unit for loss function
loss.unit <- "individual"
# Define measure for loss function
loss.measure <- "mse"
# Define lambdas as vector for Lasso
lasso.lambda.set <- c(1, 2, 3)
# Define lambdas as data frame for Lasso
lasso.lambda.set <- data.frame(step_size = c(0.1, 1),
threshold = c(1, 5))
# Define maximum number of iterations w/o improvement for Lasso
lasso.iterations.max <- NULL
# Define set of interaction depths for GB
gb.interaction.set <- c(1, 2, 3)
# Define set of learning rates for GB
gb.shrinkage.set <- c(0.04, 0.01)
# Define initial number of total trees
gb.tree.start <- 50
# Define increase in number of trees as scalar for GB
gb.tree.increase.set <- 50
# Define increase in number of trees as vector for GB
gb.tree.increase.set <- c(50, 100)
# Define maximum number of trees as scalar for GB
gb.trees.max.set <- 200
# Define maximum number of trees as vector for GB
gb.trees.max.set <- c(200, 400)
# Define maximum number of iterations w/o improvement for GB
gb.iterations.max <- NULL
# Define minimum number of observations in terminal nodes for GB
gb.n.minobsinnode <- 5
# Define whether L2.unit should be inlcuded in GB
gb.L2.unit.include <- FALSE
# Define whether L2.reg should be included in GB
gb.L2.reg.include <- FALSE
# Define kernel for SVM
svm.kernel <- "radial"
# Define error function for SVM
svm.error.fun <- "MSE"   # // might need to be changed to NULL
# Define gamma parameters for SVM
svm.gamma.set <- c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4)
# Define cost parameters for SVM
svm.cost.set <- c(1, 10)
# Define number of draws from EBMA sample with equal obs/state
Ndraws <- 100
# Define tolerance values for EBMA tuning
tol.values <- c(0.01, 0.005, 0.001)
#_______________________________________________________
# load functions
# aux functions
source("./utils.R")
# classifiers
source("./best_subset.R")
source("./best_subset_classifier.R")
source("./lasso.R")
source("./lasso_classifier.R")
source("./pca.R")
source("./gb.R")
source("./gb_classifier.R")
source("./post_stratification.R")
source("./ebma.R")
source("./svm_classifier.R")
source("./svm.R")
# Set seed
if (is.null(seed)) {
set.seed(12345)
} else {
set.seed(seed)
}
# Error and warning checks
if (!all(L1.x %in% colnames(survey))) {
stop(paste("Individual-level variable(s) '",
L1.x[which(!(L1.x %in% colnames(survey)))],
"' is/are not in your survey data.", sep = ""))
}
if (!all(L1.x %in% colnames(census))) {
stop(paste("Individual-level variable(s) '",
L1.x[which(!(L1.x %in% colnames(census)))],
"' is/are not in your census data.", sep = ""))
}
if (!all(L2.x %in% colnames(survey))) {
stop(paste("Context-level variable(s) '",
L2.x[which(!(L2.x %in% colnames(survey)))],
"' is/are not in your survey data.", sep = ""))
}
if (!all(L2.x %in% colnames(census))) {
stop(paste("Context-level variable(s) '",
L2.x[which(!(L2.x %in% colnames(census)))],
"' is/are not in your census data.", sep = ""))
}
if (!(y %in% colnames(survey))) {
stop(paste("Outcome '", y,
"' is not in your survey data.", sep = ""))
}
if (!(L2.unit %in% colnames(survey))) {
stop(paste("The geographic unit '", L2.unit,
"' is not in your survey data.", sep = ""))
}
if (!(L2.unit %in% colnames(census))) {
stop(paste("The geographic unit '", L2.unit,
"' is not in your census data.", sep = ""))
}
if (!is.null(L2.reg)) {
if (!(L2.reg %in% colnames(survey))) {
stop(paste("The geographic region '", L2.reg,
"' is not in your survey data.", sep = ""))
}
if (!(L2.reg %in% colnames(census))) {
stop(paste("The geographic region '", L2.reg,
"' is not in your census data.", sep = ""))
}
if (any(unlist(lapply(dplyr::group_split(survey, .data[[L2.unit]]),
function(x) length(unique(x[[L2.reg]])))) > 1)) {
stop(paste("The geographic unit(s) '",
which(unlist(lapply(dplyr::group_split(survey, .data[[L2.unit]]),
function(x) length(unique(x[[L2.reg]])))) > 1),
"' is/are nested in multiple regions in your survey data."))
}
if (any(unlist(lapply(dplyr::group_split(census, .data[[L2.unit]]),
function(x) length(unique(x[[L2.reg]])))) > 1)) {
stop(paste("The geographic unit(s) '",
which(unlist(lapply(dplyr::group_split(census, .data[[L2.unit]]),
function(x) length(unique(x[[L2.reg]])))) > 1),
"' is/are nested in multiple regions in your census data."))
}
}
if (is.null(ebma.size)) {
ebma.size <- round(nrow(survey) / 3, digits = 0)
} else if (is.numeric(ebma.size) & ebma.size > 0 & ebma.size < 1) {
ebma.size <- round(nrow(survey) * ebma.size, digits = 0)
} else {
stop("ebma.size must be a rational number in the open unit interval.")
}
if (!((is.integer(k.folds) | all(as.integer(k.folds) == k.folds)) &
length(k.folds) == 1)) {
stop("k.folds must be an integer number.")
}
if (!cv.sampling %in% c("respondents", "L2 units")) {
stop("cv.sampling must take either the value 'respondents' or 'L2 units'.")
}
if (!(is.vector(lasso.lambda.set) | is.data.frame(lasso.lambda.set))) {
stop(paste("lasso.lambda.set must be either a numeric vector or a data.frame ",
"with two columns, one for step size increase and the other ",
"for the upper threshold of the interval of lambdas to which ",
"the step size applies", sep = ""))
}
if (!(is.null(lasso.iterations.max) | (is.numeric(lasso.iterations.max) &
length(lasso.iterations.max) == 1))) {
stop("lasso.iterations.max must be either a numeric scalar or NULL.")
}
if (!(is.integer(gb.interaction.set) |
all(as.integer(gb.interaction.set) == gb.interaction.set))) {
stop("gb.interaction.set must be an integer-valued vector.")
}
if (!is.numeric(gb.shrinkage.set)) {
stop("gb.shrinkage.set must be a numeric vector")
} else if (min(gb.shrinkage.set) < 0.001 | max(gb.shrinkage.set) > 0.1) {
warning("gb.shrinkage.set should have values lying between 0.001 and 0.1.")
}
if (!((is.integer(gb.tree.start) |
all(as.integer(gb.tree.start) == gb.tree.start)) &
length(gb.tree.start) == 1)) {
stop("gb.tree.start must be an integer-valued scalar.")
}
if (!(is.integer(gb.tree.increase.set) |
all(as.integer(gb.tree.increase.set) == gb.tree.increase.set))) {
stop("gb.tree.increase.set must be an integer-valued scalar or vector.")
} else if (length(gb.tree.increase.set) > 1 &
length(gb.tree.increase.set) != length(gb.shrinkage.set)) {
stop(paste("gb.tree.increase.set must be either a scalar or a vector of ",
"size `length(gb.shrinkage.set)`.", sep = ""))
}
if (!(is.integer(gb.trees.max.set) |
all(as.integer(gb.trees.max.set) == gb.trees.max.set))) {
stop("gb.trees.max.set must be an integer-valued scalar or vector.")
} else if (length(gb.trees.max.set) > 1 &
length(gb.trees.max.set) != length(gb.shrinkage.set)) {
stop(paste("gb.trees.max.set must be either a scalar or a vector of size ",
"`length(gb.shrinkage.set)`.", sep = ""))
}
# ------------------------------- Prepare data -------------------------------
# If not provided in census data, calculate bin size for each ideal type in
# a geographic unit
if (is.null(bin.size)) {
census <- census %>%
dplyr::group_by(.dots = c(L1.x, L2.unit)) %>%
dplyr::summarise(n = dplyr::n())
} else {
census$n <- census[[bin.size]]
}
# In census data, calculate bin proportion for each ideal type in a
# geographic unit
census <- census %>%
dplyr::group_by(.dots = L2.unit) %>%
dplyr::mutate(prop = n / sum(n))
# Scale context-level variables in survey and census data
survey[, L2.x] <- scale(survey[, L2.x], center = TRUE, scale = TRUE)
census[, L2.x] <- scale(census[, L2.x], center = TRUE, scale = TRUE)
# Compute principal components for survey data
pca_out <- stats::prcomp(survey[, L2.x],
retx = TRUE,
center = TRUE,
scale. = TRUE,
tol = NULL)
# Add PCs to survey data
survey <- survey %>%
dplyr::bind_cols(as.data.frame(pca_out$x))
# Add PCs to census data
pc_names <- colnames(pca_out$x)
census <- census %>%
dplyr::left_join(unique(dplyr::select(survey, L2.unit, pc_names)),
by = L2.unit)
# ------------------------------- Create folds -------------------------------
# EBMA hold-out fold
ebma_folding_out <- ebma_folding(data = survey,
L2.unit = L2.unit,
ebma.size = ebma.size)
ebma_fold <- ebma_folding_out$ebma_fold
cv_data <- ebma_folding_out$cv_data
# K folds for cross-validation
cv_folds <- cv_folding(data = cv_data,
L2.unit = L2.unit,
k.folds = k.folds,
cv.sampling = cv.sampling)
# ------------------------ Run individual classifiers ------------------------
# Classifier 1: Best Subset
best_subset_out <- best_subset(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.measure = loss.measure,
data = cv_folds,
verbose = verbose)
# Classifier 2: Lasso
lasso_out <- lasso(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.measure = loss.measure,
lambda.set = lasso.lambda.set,
iterations.max = lasso.iterations.max,
data = cv_folds,
verbose = verbose)
# Classifier 3: PCA
pca_out <- pca(y = y,
L1.x = L1.x,
L2.x = pc_names,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.measure = loss.measure,
data = cv_folds,
verbose = verbose)
# Classifier 4: GB
gb_out <- gb(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
L2.unit.include = gb.L2.unit.include,
L2.reg.include = gb.L2.reg.include,
loss.unit = loss.unit,
loss.measure = loss.measure,
interaction.set = gb.interaction.set,
shrinkage.set = gb.shrinkage.set,
tree.start = gb.tree.start,
tree.increase.set = gb.tree.increase.set,
trees.max.set = gb.trees.max.set,
iterations.max = gb.iterations.max,
n.minobsinnode = gb.n.minobsinnode,
data = cv_folds,
verbose = verbose)
# Classifier 5: SVM
svm_out <- svm(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
kernel = svm.kernel,
error.fun = svm.error.fun,
gamma.set = svm.gamma.set,
cost.set = svm.cost.set,
k.folds = k.folds,
data = cv_folds,
verbose = verbose)
# --------------------------- Post-stratification ----------------------------
ps_out <- post_stratification(data = cv_folds,
census = census,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
best.subset = best_subset_out,
pca = pca_out,
lasso = lasso_out,
gb = gb_out,
n.minobsinnode = gb.n.minobsinnode,
L2.unit.include = gb.L2.unit.include,
L2.reg.include = gb.L2.reg.include,
svm.out = svm_out,
kernel = svm.kernel,
verbose = verbose)
# ----------------------------------- EBMA ------------------------------------
ebma.out <- ebma(ebma.fold = ebma_fold,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
post.strat = ps_out,
Ndraws = Ndraws,
tol.values = tol.values,
best.subset = best_subset_out,
pca = pca_out,
lasso = lasso_out,
gb = gb_out,
svm.out = svm_out,
verbose = verbose)
print(ebma.out, n = nrow(ebma.out))

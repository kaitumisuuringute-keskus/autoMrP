#'   \code{NULL}. \emph{Note:} if \code{bin.proportion} is \code{NULL}, then
#'   \code{bin.size} must be specified.
#' @param bin.size Bin size of ideal types. A character scalar containing the
#'   column name of the variable in \code{census} that indicates the bin size of
#'   ideal types by geographic unit. Default is \code{NULL}. \emph{Note:}
#'   ignored if \code{bin.proportion} is provided, but must be specified
#'   otherwise.
#' @param survey Survey data. A \code{data.frame} whose column names include
#'   \code{y}, \code{L1.x}, \code{L2.x}, \code{L2.unit}, and, if specified,
#'   \code{L2.reg}, \code{pcs}, and \code{folds}.
#' @param census Census data. A \code{data.frame} whose column names include
#'   \code{L1.x}, \code{L2.x}, \code{L2.unit}, if specified, \code{L2.reg} and
#'   \code{pcs}, and either \code{bin.proportion} or \code{bin.size}.
#' @param ebma.size EBMA fold size. A number in the open unit interval
#'   indicating the share of respondents to be allocated to the EBMA fold.
#'   Default is \eqn{1/3}. \emph{Note:} ignored if \code{folds} is provided, but
#'   must be specified otherwise.
#' @param k.folds Number of cross-validation folds. An integer-valued scalar
#'   indicating the number of folds to be used in cross-validation. Default is
#'   \eqn{5}. \emph{Note:} ignored if \code{folds} is provided, but must be
#'   specified otherwise.
#' @param cv.sampling Cross-validation sampling method. A character-valued
#'   scalar indicating whether cross-validation folds should be created by
#'   sampling individual respondents (\code{individuals}) or geographic units
#'   (\code{L2 units}). Default is \code{L2 units}. \emph{Note:} ignored if
#'   \code{folds} is provided, but must be specified otherwise.
#' @param loss.unit Loss function unit. A character-valued scalar indicating
#'   whether performance loss should be evaluated at the level of individual
#'   respondents (\code{individuals}) or geographic units (\code{L2 units}).
#'   Default is \code{individuals}.
#' @param loss.fun Loss function. A character-valued scalar indicating whether
#'   prediction loss should be measured by the mean squared error (\code{MSE})
#'   or the mean absolute error (\code{MAE}). Default is \code{MSE}.
#' @param best.subset Best subset classifier. A logical argument indicating
#'   whether the best subset classifier should be used for predicting outcome
#'   \code{y}. Default is \code{TRUE}.
#' @param lasso Lasso classifier. A logical argument indicating whether the
#'   lasso classifier should be used for predicting outcome \code{y}. Default is
#'   \code{TRUE}.
#' @param pca PCA classifier. A logical argument indicating whether the PCA
#'   classifier should be used for predicting outcome \code{y}. Default is
#'   \code{TRUE}.
#' @param gb GB classifier. A logical argument indicating whether the GB
#'   classifier should be used for predicting outcome \code{y}. Default is
#'   \code{TRUE}.
#' @param svm SVM classifier. A logical argument indicating whether the SVM
#'   classifier should be used for predicting outcome \code{y}. Default is
#'   \code{TRUE}.
#' @param mrp MRP classifier. A logical argument indicating whether the standard
#'   MRP classifier should be used for predicting outcome \code{y}. Default is
#'   \code{FALSE}.
#' @param forward.select Forward selection classifier. A logical argument
#'   indicating whether to use forward selection rather than best subset
#'   selection. Default is \code{FALSE}. \emph{Note:} forward selection is
#'   recommended if there are more than \eqn{8} context-level variables.
#' @param best.subset.L2.x Best subset context-level covariates. A character
#'   vector containing the column names of the context-level variables in
#'   \code{survey} and \code{census} to be used by the best subset classifier.
#'   If \code{NULL} and \code{best.subset} is set to \code{TRUE}, then best
#'   subset uses the variables specified in \code{L2.x}. Default is \code{NULL}.
#' @param lasso.L2.x Lasso context-level covariates. A character vector
#'   containing the column names of the context-level variables in
#'   \code{survey} and \code{census} to be used by the lasso classifier. If
#'   \code{NULL} and \code{lasso} is set to \code{TRUE}, then lasso uses the
#'   variables specified in \code{L2.x}. Default is \code{NULL}.
#' @param gb.L2.x GB context-level covariates. A character vector containing the
#'   column names of the context-level variables in \code{survey} and
#'   \code{census} to be used by the GB classifier. If \code{NULL} and \code{gb}
#'   is set to \code{TRUE}, then GB uses the variables specified in \code{L2.x}.
#'   Default is \code{NULL}.
#' @param svm.L2.x SVM context-level covariates. A character vector containing
#'   the column names of the context-level variables in \code{survey} and
#'   \code{census} to be used by the SVM classifier. If \code{NULL} and
#'   \code{svm} is set to \code{TRUE}, then SVM uses the variables specified in
#'   \code{L2.x}. Default is \code{NULL}.
#' @param mrp.L2.x MRP context-level covariates. A character vector containing
#'   the column names of the context-level variables in \code{survey} and
#'   \code{census} to be used by the MRP classifier. If \code{NULL} and
#'   \code{mrp} is set to \code{TRUE}, then MRP uses the variables specified in
#'   \code{L2.x}. Default is \code{NULL}.
#' @param gb.L2.unit GB L2.unit. A logical argument indicating whether L2.unit
#'   should be included in the GB classifier. Default is \code{FALSE}.
#' @param gb.L2.reg GB L2.reg. A logical argument indicating whether L2.reg
#'   should be included in the GB classifier. Default is \code{FALSE}.
#' @param lasso.lambda Lasso penalty parameter. A numeric \code{vector} of
#'   non-negative values or a \code{list} of two numeric vectors of equal size,
#'   with the first vector containing the step sizes by which the penalty
#'   parameter should increase and the second vector containing the upper
#'   thresholds of the intervals to which the step sizes apply. The penalty
#'   parameter controls the shrinkage of the context-level variables in the
#'   lasso model. Default is \code{list(c(0.1, 0.3, 1), c(1, 10, 10000))}.
#' @param lasso.n.iter Lasso number of iterations without improvement. Either
#'   \code{NULL} or an integer-valued scalar specifying the maximum number of
#'   iterations without performance improvement the algorithm runs before
#'   stopping. Default is \eqn{70}.
#' @param gb.interaction.depth GB interaction depth. An integer-valued vector
#'   whose values specify the interaction depth of GB. The interaction depth
#'   defines the maximum depth of each tree grown (i.e., the maximum level of
#'   variable interactions). Default is \code{c(1, 2, 3)}.
#' @param gb.shrinkage GB learning rate. A numeric vector whose values specify
#'   the learning rate or step-size reduction of GB. Values between \eqn{0.001}
#'   and \eqn{0.1} usually work, but a smaller learning rate typically requires
#'   more trees. Default is \code{c(0.04, 0.01, 0.008, 0.005, 0.001)}.
#' @param gb.n.trees.init GB initial total number of trees. An integer-valued
#'   scalar specifying the initial number of total trees to fit by GB. Default
#'   is \eqn{50}.
#' @param gb.n.trees.increase GB increase in total number of trees. An
#'   integer-valued scalar specifying by how many trees the total number of
#'   trees to fit should be increased (until \code{gb.n.trees.max} is reached)
#'   or an integer-valued vector of length \code{length(gb.shrinkage)} with each
#'   of its values being associated with a learning rate in \code{gb.shrinkage}.
#'   Default is \eqn{50}.
#' @param gb.n.trees.max GB maximum number of trees. An integer-valued scalar
#'   specifying the maximum number of trees to fit by GB or an integer-valued
#'   vector of length \code{length(gb.shrinkage)} with each of its values being
#'   associated with a learning rate and an increase in the total number of
#'   trees. Default is \eqn{1000}.
#' @param gb.n.iter GB number of iterations without improvement. A numeric
#'   scalar specifying the maximum number of iterations without performance
#'   improvement the algorithm runs before stopping. Default is \eqn{70}.
#' @param gb.n.minobsinnode GB minimum number of observations in the terminal
#'   nodes. An integer-valued scalar specifying the minimum number of
#'   observations that each terminal node of the trees must contain. Default is
#'   \eqn{5}.
#' @param svm.kernel SVM kernel. A character-valued scalar specifying the kernel
#'   to be used by SVM. The possible values are \code{linear}, \code{polynomial},
#'   \code{radial}, and \code{sigmoid}. Default is \code{radial}.
#' @param svm.loss.fun SVM loss function. If \code{NULL}, then SVM uses the
#'   misclassification error to measure the loss of categorical predictions and
#'   the mean squared error to measure the loss of numeric predictions. Default
#'   is \code{NULL}.
#' @param svm.gamma SVM kernel parameter. A numeric vector whose values specify
#'   the gamma parameter in the SVM kernel. This parameter is needed for all
#'   kernel types except linear. Default is
#'   \eqn{c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4)}.
#' @param svm.cost SVM cost parameter. A numeric vector whose values specify the
#'   cost of constraints violation in SVM. Default is \eqn{c(1, 10)}.
#' @param ebma.n.draws EBMA number of samples. An integer-valued scalar
#'   specifying the number of bootstrapped samples to be drawn from the EBMA
#'   fold and used for tuning EBMA. Default is \eqn{100}.
#' @param ebma.tol EBMA tolerance. A numeric vector containing the
#'   tolerance values for improvements in the log-likelihood before the EM
#'   algorithm stops optimization. Values should range at least from \eqn{0.01}
#'   to \eqn{0.001}. Default is
#'   \code{c(0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001)}.
#' @param uncertainty Uncertainty estimates. A logical argument indicating
#'   whether uncertainty estimates should be computed. Default is \code{FALSE}.
#' @param seed Seed. Either \code{NULL} or an integer-valued scalar controlling
#'   random number generation. If \code{NULL}, then the seed is set to
#'   \eqn{546213978}. Default is \code{NULL}.
#' @param verbose Verbose output. A logical argument indicating whether or not
#'   verbose output should be printed. Default is \code{TRUE}.
#' @return
#' @keywords MRP multilevel regression post-stratification machine learning
#'   EBMA ensemble bayesian model averaging
#' @examples
#' @export
auto_MrP <- function(y, L1.x, L2.x, L2.unit, L2.reg = NULL, L2.x.scale = TRUE,
pcs = NULL, folds = NULL, bin.proportion = NULL,
bin.size = NULL, survey, census, ebma.size = 1/3,
k.folds = 5, cv.sampling = "L2 units",
loss.unit = "individuals", loss.fun = "MSE",
best.subset = TRUE, lasso = TRUE, pca = TRUE, gb = TRUE,
svm = TRUE, mrp = FALSE, forward.select = FALSE,
best.subset.L2.x = NULL, lasso.L2.x = NULL,
gb.L2.x = NULL, svm.L2.x = NULL, mrp.L2.x = NULL,
gb.L2.unit = FALSE, gb.L2.reg = FALSE,
lasso.lambda = list(c(0.1, 0.3, 1), c(1, 10, 10000)),
lasso.n.iter = 70, gb.interaction.depth = c(1, 2, 3),
gb.shrinkage = c(0.04, 0.01, 0.008, 0.005, 0.001),
gb.n.trees.init = 50, gb.n.trees.increase = 50,
gb.n.trees.max = 1000, gb.n.iter = 70,
gb.n.minobsinnode = 5, svm.kernel = "radial",
svm.loss.fun = NULL, svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65,
0.7, 0.8, 0.9, 1, 2, 3, 4),
svm.cost = c(1, 10), ebma.n.draws = 100,
ebma.tol = c(0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005,
0.00001), uncertainty = FALSE, seed = NULL,
verbose = TRUE) {
# ----------------------------------- Seed -----------------------------------
# Check seed argument and set seed
if (is.null(seed)) {
set.seed(546213978)
} else {
if (isTRUE(dplyr::near(seed, as.integer(seed)))) {
set.seed(seed)
} else {
stop("Seed must be either NULL or an integer-valued scalar.")
}
}
# ------------------------------- Error checks -------------------------------
error_checks(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
L2.x.scale = L2.x.scale,
pcs = pcs,
folds = folds,
bin.proportion = bin.proportion,
bin.size = bin.size,
survey = survey,
census = census,
ebma.size = ebma.size,
k.folds = k.folds,
cv.sampling = cv.sampling,
loss.unit = loss.unit,
loss.fun = loss.fun,
best.subset = best.subset,
lasso = lasso,
pca = pca,
gb = gb,
svm = svm,
mrp = mrp,
forward.select = forward.select,
best.subset.L2.x = best.subset.L2.x,
lasso.L2.x = lasso.L2.x,
gb.L2.x = gb.L2.x,
svm.L2.x = svm.L2.x,
mrp.L2.x = mrp.L2.x,
gb.L2.unit = gb.L2.unit,
gb.L2.reg = gb.L2.reg,
lasso.lambda = lasso.lambda,
lasso.n.iter = lasso.n.iter)
if (is.null(ebma.size)) {
ebma.size <- round(nrow(survey) / 3, digits = 0)
} else if (is.numeric(ebma.size) & ebma.size > 0 & ebma.size < 1) {
ebma.size <- round(nrow(survey) * ebma.size, digits = 0)
} else {
stop("ebma.size must be a rational number in the open unit interval.")
}
if (!(is.null(lasso.iterations.max) | (is.numeric(lasso.iterations.max) &
length(lasso.iterations.max) == 1))) {
stop("lasso.iterations.max must be either a numeric scalar or NULL.")
}
if (!(is.integer(gb.interaction.set) |
all(as.integer(gb.interaction.set) == gb.interaction.set))) {
stop("gb.interaction.set must be an integer-valued vector.")
}
if (!is.numeric(gb.shrinkage.set)) {
stop("gb.shrinkage.set must be a numeric vector")
} else if (min(gb.shrinkage.set) < 0.001 | max(gb.shrinkage.set) > 0.1) {
warning("gb.shrinkage.set should have values lying between 0.001 and 0.1.")
}
if (!((is.integer(gb.tree.start) |
all(as.integer(gb.tree.start) == gb.tree.start)) &
length(gb.tree.start) == 1)) {
stop("gb.tree.start must be an integer-valued scalar.")
}
if (!(is.integer(gb.tree.increase.set) |
all(as.integer(gb.tree.increase.set) == gb.tree.increase.set))) {
stop("gb.tree.increase.set must be an integer-valued scalar or vector.")
} else if (length(gb.tree.increase.set) > 1 &
length(gb.tree.increase.set) != length(gb.shrinkage.set)) {
stop(paste("gb.tree.increase.set must be either a scalar or a vector of ",
"size `length(gb.shrinkage.set)`.", sep = ""))
}
if (!(is.integer(gb.trees.max.set) |
all(as.integer(gb.trees.max.set) == gb.trees.max.set))) {
stop("gb.trees.max.set must be an integer-valued scalar or vector.")
} else if (length(gb.trees.max.set) > 1 &
length(gb.trees.max.set) != length(gb.shrinkage.set)) {
stop(paste("gb.trees.max.set must be either a scalar or a vector of size ",
"`length(gb.shrinkage.set)`.", sep = ""))
}
# ------------------------------- Prepare data -------------------------------
# Coerce individual-level variables and geographic variables to factors in
# survey and census data
survey <- survey %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), .funs = as.factor)
census <- census %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), .funs = as.factor)
# If not provided in census data, calculate bin size and bin proportion for
# each ideal type in a geographic unit
if (is.null(bin.proportion)) {
if (is.null(bin.size)) {
census <- census %>%
dplyr::group_by(.dots = c(L1.x, L2.unit)) %>%
dplyr::summarise(n = dplyr::n())
} else {
census$n <- census[[bin.size]]
}
census <- census %>%
dplyr::group_by(.dots = L2.unit) %>%
dplyr::mutate(prop = n / sum(n))
} else {
census <- census %>%
dplyr::rename(prop = one_of(bin.proportion))
}
# If not provided in survey and census data, compute the principals components
# of the context-level variables
if (is.null(pcs)) {
# Compute principal components for survey data
pca_out <- stats::prcomp(survey[, L2.x],
retx = TRUE,
center = TRUE,
scale. = TRUE,
tol = NULL)
# Add PCs to survey data
survey <- survey %>%
dplyr::bind_cols(as.data.frame(pca_out$x))
# Add PCs to census data
pc_names <- colnames(pca_out$x)
census <- census %>%
dplyr::left_join(unique(survey %>% dplyr::select(all_of(L2.unit),
all_of(pc_names))),
by = L2.unit)
} else {
pc_names <- pcs
}
# Scale context-level variables in survey and census data
if (isTRUE(L2.x.scale)) {
survey[, L2.x] <- scale(survey[, L2.x], center = TRUE, scale = TRUE)
census[, L2.x] <- scale(census[, L2.x], center = TRUE, scale = TRUE)
}
# Convert survey and census data to tibble
survey <- tibble::as_tibble(x = survey)
census <- tibble::as_tibble(x = census)
# ------------------------------- Create folds -------------------------------
if (is.null(folds)) {
# EBMA hold-out fold
ebma_folding_out <- ebma_folding(data = survey,
L2.unit = L2.unit,
ebma.size = ebma.size)
ebma_fold <- ebma_folding_out$ebma_fold
cv_data <- ebma_folding_out$cv_data
# K folds for cross-validation
cv_folds <- cv_folding(data = cv_data,
L2.unit = L2.unit,
k.folds = k.folds,
cv.sampling = cv.sampling)
} else {
# EBMA hold-out fold
ebma_fold <- survey %>%
dplyr::filter_at(dplyr::vars(dplyr::one_of(folds)),
dplyr::any_vars(. == k.folds + 1))
# K folds for cross-validation
cv_data <- survey %>%
dplyr::filter_at(dplyr::vars(dplyr::one_of(folds)),
dplyr::any_vars(. != k.folds + 1))
cv_folds <- cv_data %>%
dplyr::group_split(.data[[folds]])
}
# ---------------------- Optimal individual classifiers ----------------------
# Classifier 1: Best Subset
if (isTRUE(best.subset)) {
best_subset_out <- run_best_subset(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.measure = loss.measure,
data = cv_folds,
verbose = verbose)
} else {
best_subset_out <- NULL
}
# Classifier 2: Lasso
if (isTRUE(lasso)) {
lasso_out <- run_lasso(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.measure = loss.measure,
lambda.set = lasso.lambda.set,
iterations.max = lasso.iterations.max,
data = cv_folds,
verbose = verbose)
} else {
lasso_out <- NULL
}
# Classifier 3: PCA
if (isTRUE(pca)) {
pca_out <- run_pca(y = y,
L1.x = L1.x,
L2.x = pc_names,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.measure = loss.measure,
data = cv_folds,
verbose = verbose)
} else {
pca_out <- NULL
}
# Classifier 4: GB
if (isTRUE(gb)) {
gb_out <- run_gb(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
L2.unit.include = gb.L2.unit.include,
L2.reg.include = gb.L2.reg.include,
loss.unit = loss.unit,
loss.measure = loss.measure,
interaction.set = gb.interaction.set,
shrinkage.set = gb.shrinkage.set,
tree.start = gb.tree.start,
tree.increase.set = gb.tree.increase.set,
trees.max.set = gb.trees.max.set,
iterations.max = gb.iterations.max,
n.minobsinnode = gb.n.minobsinnode,
data = cv_folds,
verbose = verbose)
} else {
gb_out <- NULL
}
# Classifier 5: SVM
if (isTRUE(svm)) {
svm_out <- run_svm(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
kernel = svm.kernel,
error.fun = svm.error.fun,
gamma.set = svm.gamma.set,
cost.set = svm.cost.set,
k.folds = k.folds,
data = cv_folds,
verbose = verbose)
} else {
svm_out <- NULL
}
# --------------------------- Post-stratification ----------------------------
ps_out <- post_stratification(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
best.subset.opt = best_subset_out,
lasso.opt = lasso_out,
pca.opt = pca_out,
gb.opt = gb_out,
svm.opt = svm_out,
mrp.include = mrp.include,
n.minobsinnode = gb.n.minobsinnode,
L2.unit.include = gb.L2.unit.include,
L2.reg.include = gb.L2.reg.include,
kernel = svm.kernel,
L2.x.mrp = mrp.L2.x,
data = cv_data,
census = census,
verbose = verbose)
# ----------------------------------- EBMA -----------------------------------
ebma_out <- ebma(ebma.fold = ebma_fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
post.strat = ps_out,
Ndraws = ebma.n.draws,
tol.values = ebma.tol.values,
best.subset = best_subset_out,
pca = pca_out,
lasso = lasso_out,
gb = gb_out,
svm.out = svm_out,
verbose = verbose)
# ----------------------------- Function output ------------------------------
return(ebma_out)
}
install.packages("roxygen2")
install.packages(c("dplyr", "tibble"))
install.packages("lme4")
install.packages(c("dplyr", "gbm"))
#e
packrat::init(options = list(auto.snapshot = TRUE))
install.packages("lme4")
install.packages("gbm")
install.packages("glmmLasso")
install.packages("e1071")
library(autoMrP)
install.packages("EBMAforecast")
packageurl <- "https://cran.r-project.org/src/contrib/Archive/EBMAforecast/EBMAforecast_0.52.tar.gz"
install.packages(packageurl, repos = NULL, type = "source")
install.packages("separationplot")
install.packages(packageurl, repos = NULL, type = "source")
install.packages("plyr")
install.packages("abind")
install.packages(packageurl, repos = NULL, type = "source")
library(autoMrP)
class(glmm)
??predict.merMod
lme4::predict.merMod
predict.merMod()
lme4::predict.merMod()
lme4::predict()
lme4::predict.merMod
lme4::predict
packrat::init(options = list(auto.snapshot = TRUE))
install.packages("https://cran.r-project.org/src/contrib/Archive/EBMAforecast/EBMAforecast_0.52.tar.gz", repos = NULL, type = "source")
install.packages("separationplot")
install.packages("https://cran.r-project.org/src/contrib/Archive/EBMAforecast/EBMAforecast_0.52.tar.gz", repos = NULL, type = "source")
install.packages("plyr")
install.packages("abind")
install.packages("https://cran.r-project.org/src/contrib/Archive/EBMAforecast/EBMAforecast_0.52.tar.gz", repos = NULL, type = "source")
install.packages("glmmLasso")
install.packages(c("e1071", "gbm", "lme4"))
install.packages("gbm")
install.packages("rlang")
install.packages(c("boot", "class", "foreign", "KernSmooth", "MASS", "nnet", "spatial"), lib="C:/Users/Philipp/Documents/github/autoMrP/packrat/lib-R/x86_64-w64-mingw32/4.0.0")
install.packages("lme4")
install.packages("magrittr")
library(autoMrP)
library(autoMrP)
packrat::set_opts(vcs.ignore.src = TRUE, use.cache = TRUE)

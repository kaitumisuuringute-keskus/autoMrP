model = "normal",
useModelParams = FALSE,
tol = tol[idx.tol]))
}
# mse
mse_collector[idx.Ndraws, idx.tol] <- mean(( as.numeric(unlist(test_y)) -
as.numeric( attributes(forecast.out)$predTest[,1,1]))^2)
# model weights
weights_box[idx.Ndraws, , idx.tol] <- attributes(forecast.out)$modelWeights
# progress
if (verbose) cat(paste("\n","EBMA: ", round(counter / (length(tol) * n.draws),2)*100, "% done",sep=""))
}
}
library(autoMrP)
devtools::load_all()
rm(list = c("ebma_mc_tol"))
devtools::load_all()
set.seed(seed)
ebma_out <- ebma(
ebma.fold = ebma_fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
pc.names = pc_names,
post.strat = ps_out,
n.draws = ebma.n.draws,
tol = ebma.tol,
best.subset.opt = best_subset_out,
pca.opt = pca_out,
lasso.opt = lasso_out,
gb.opt = gb_out,
svm.opt = svm_out,
verbose = verbose,
cores = cores
)
ebma_out
message("Starting bayesian ensemble model averaging tuning")
# EBMA wihtout L2.x variables
if (all(L2.x == "")) L2.x <- NULL
# Models
model_bs <- post.strat$models$best_subset
model_pca <- post.strat$models$pca
model_lasso <- post.strat$models$lasso
model_gb <- post.strat$models$gb
model_svm <- post.strat$models$svm
model_mrp <- post.strat$models$mrp
# Training predictions
train_preds <- post.strat$predictions$Level1 %>%
dplyr::select(-one_of(y))
# Training set outcomes
train_y <- dplyr::pull(.data = post.strat$predictions$Level1, var = y)
# Parallel tuning, if cores > 1
if (cores > 1){
# Distribute clusters over tolerance values or n.draws
if(length(tol) <= n.draws*3){
final_model_weights <- ebma_mc_draws(
train.preds = train_preds,
train.y = train_y,
ebma.fold = ebma.fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
pc.names = pc.names,
model.bs = model_bs,
model.pca = model_pca,
model.lasso = model_lasso,
model.gb = model_gb,
model.svm = model_svm,
model.mrp = model_mrp,
tol = tol,
n.draws = n.draws,
cores = cores)
} else {
final_model_weights <- ebma_mc_tol(
train.preds = train_preds,
train.y = train_y,
ebma.fold = ebma.fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
pc.names = pc.names,
model.bs = model_bs,
model.pca = model_pca,
model.lasso = model_lasso,
model.gb = model_gb,
model.svm = model_svm,
model.mrp = model_mrp,
tol = tol,
n.draws = n.draws,
cores = cores)
}
} else{
# Counter for verbose screen output
counter <- 0
# Container to store the MSE on the test folds
# Bootstrap draws in rows and tolerance values in columns
mse_collector <- matrix(
NA, nrow = n.draws,
ncol = length(tol),
dimnames = list( c(paste0("Ndraw_", seq(1:n.draws))),
c(paste0("Tol: ", tol) )))
# container for model weights for each draw and tolerance value
# Dimension 1 (rows): Bootstrap draws
# Dimension 2 (columns): Classifiers
# Dimension 3 (layers): Tolerance values
weights_box <- array(
NA,
dim = c(n.draws, ncol(train_preds), length(tol)),
dimnames = list( c(paste0("Ndraw_", seq(1:n.draws))),
c(colnames(train_preds)),
c(paste0("Tol: ", tol))))
# loop over tolerance values
for (idx.tol in 1:length(tol)){
# loop over Ndraws wit equal obs/state
for (idx.Ndraws in 1:n.draws){
# Increase counter
counter <- counter +1
# Determine number per group to sample
n_per_group <- as.integer(nrow(ebma.fold) / length(levels(ebma.fold[[L2.unit]])))
# Test set with n_per_group persons per state (with resampling)
test <- ebma.fold %>%
dplyr::group_by_at( .vars = L2.unit ) %>%
dplyr::sample_n( n_per_group, replace = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate_at(.vars = c( L1.x, L2.unit, L2.reg), .funs = as.factor) %>%
dplyr::select( dplyr::one_of(c(y, L1.x, L2.x, L2.unit, L2.reg, pc.names))) %>%
tidyr::drop_na()
# predict outcomes in test set
test_preds <- dplyr::tibble(
best_subset = if(!is.null(model_bs)){
predict(object = model_bs, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA},
pca = if(!is.null(model_pca)){
predict(object = model_pca, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA},
lasso = if(!is.null(model_lasso)){
predict_glmmLasso(census = test, m = model_lasso, L1.x = L1.x, lasso.L2.x = L2.x, L2.unit = L2.unit, L2.reg = L2.reg)
} else{NA},
gb = if(!is.null(model_gb)){
gbm::predict.gbm(object = model_gb, newdata = test, n.trees = model_gb$n.trees, type = "response")
} else{NA},
svm = if(!is.null(model_svm)){
as.numeric(attr(predict(object = model_svm, newdata = test, probability = TRUE),"probabilities")[,"1"])
} else{NA},
mrp = if(!is.null(model_mrp)){
predict(object = model_mrp, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA}
)
# remove NA's
test_preds <- test_preds[,apply(X = test_preds, MARGIN = 2, FUN = function(x){
all(!is.na(x))})]
# outcome on the test
# test_y <- dplyr::select(.data = test, one_of(y))
test_y <- dplyr::pull(.data = test, y)
# EBMA
if(verbose){
forecast.data <- EBMAforecast::makeForecastData(
.predCalibration = data.frame(train_preds),
.outcomeCalibration = train_y,
.predTest = data.frame(test_preds),
.outcomeTest = test_y)
forecast.out <- EBMAforecast::calibrateEnsemble(
forecast.data,
model = "normal",
useModelParams = FALSE,
tol = tol[idx.tol])
} else {
forecast.data <- quiet(
EBMAforecast::makeForecastData(
.predCalibration = data.frame(train_preds),
.outcomeCalibration = as.numeric(unlist(train_y)),
.predTest = data.frame(test_preds),
.outcomeTest = as.numeric(unlist(test_y)))
)
forecast.out <- quiet(EBMAforecast::calibrateEnsemble(
forecast.data,
model = "normal",
useModelParams = FALSE,
tol = tol[idx.tol]))
}
# mse
mse_collector[idx.Ndraws, idx.tol] <- mean(( as.numeric(unlist(test_y)) -
as.numeric( attributes(forecast.out)$predTest[,1,1]))^2)
# model weights
weights_box[idx.Ndraws, , idx.tol] <- attributes(forecast.out)$modelWeights
# progress
if (verbose) cat(paste("\n","EBMA: ", round(counter / (length(tol) * n.draws),2)*100, "% done",sep=""))
}
}
# which tolerance value minimizes the mse on the test set
best_tolerance <- apply(mse_collector, 1, function(x) which.min(x))
# container of best model weights
weights_mat <- matrix(data = NA, nrow = n.draws, ncol = ncol(train_preds))
# model weights; rows = observations, columns = model weights, layers = tolerance values
if (length(tol)>1){
for (idx.tol in 1:length(best_tolerance)){
weights_mat[idx.tol, ] <- weights_box[idx.tol, ,][ ,best_tolerance[idx.tol]]
}
} else{
weights_mat <- weights_box[, , 1]
}
# average model weights
if (is.null(dim(weights_mat))){
final_model_weights <- as.numeric(weights_mat)
} else{
final_model_weights <- apply(weights_mat, 2, mean)
}
names(final_model_weights) <- names(train_preds)
}
as.matrix(post.strat$predictions$Level2[,names(final_model_weights)])
final_model_weights
as.matrix(post.strat$predictions$Level2[,names(final_model_weights)])
# weighted average
model_preds <- as.matrix(post.strat$predictions$Level2[,names(final_model_weights)])
model_preds
is.na(model_preds)
model_preds[[1]]
model_preds[[is.na(model_preds)]]
which(is.na(model_preds))
model_preds[is.na(model_preds)]
model_preds[is.na(model_preds)] <- 0
model_preds[is.na(model_preds)] <- 0
model_preds
w_avg <- as.numeric(model_preds %*% final_model_weights)
w_avg
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
L2_preds
post.strat$predictions$Level2
# weighted average
model_preds <- as.matrix(post.strat$predictions$Level2[,names(final_model_weights)])
model_preds[is.na(model_preds)] <- 1
w_avg <- as.numeric(model_preds %*% final_model_weights)
w_avg
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
L2_preds
final_model_weights
# weighted average
model_preds <- as.matrix(post.strat$predictions$Level2[,names(final_model_weights)])
model_preds
model_preds[6,]
final_model_weights
model_preds[6,] * final_model_weights
model_preds[6,] * final_model_weights
model_preds
model_preds[6,]
final_model_weights
model_preds[6,] * final_model_weights
model_preds[is.na(model_preds)] <- 0
model_preds[6,] * final_model_weights
sum(model_preds[6,] * final_model_weights)
# weighted average
model_preds <- as.matrix(post.strat$predictions$Level2[,names(final_model_weights)])
sum(model_preds[6,] * final_model_weights, na.rm = TRUE)
model_preds[is.na(model_preds)] <- 0
sum(model_preds[6,] * final_model_weights, na.rm = TRUE)
library(autoMrP)
devtools::load_all()
set.seed(seed)
ebma_out <- ebma(
ebma.fold = ebma_fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
pc.names = pc_names,
post.strat = ps_out,
n.draws = ebma.n.draws,
tol = ebma.tol,
best.subset.opt = best_subset_out,
pca.opt = pca_out,
lasso.opt = lasso_out,
gb.opt = gb_out,
svm.opt = svm_out,
verbose = verbose,
cores = cores
)
ebma_out
model_preds
w_avg <- as.numeric(model_preds %*% final_model_weights)
w_avg
# weighted average
model_preds <- as.matrix(post.strat$predictions$Level2[,names(final_model_weights)])
w_avg <- as.numeric(model_preds %*% final_model_weights)
w_avg
# deal with missings
is.na(w_avg)
# deal with missings
any(is.na(w_avg))
# missing row indexes
m_idx <- which(is.na(w_avg))
m_idx
model_preds
# remove columns with NAs
no_Na_preds <- model_preds[,apply(X = model_preds, MARGIN = 2, FUN = function(x){
any(!is.na(x))})]
no_Na_preds
# remove columns with NAs
no_Na_preds <- model_preds[,apply(X = model_preds, MARGIN = 2, FUN = function(x){
all(!is.na(x))})]
no_Na_preds
final_model_weights
model_preds
# which classifiers contain missings
apply(model_preds, 2, is.na)
# which classifiers contain missings
apply(model_preds, 2, function(x) any(is.na(x)) )
# which classifiers contain missings
which(apply(model_preds, 2, function(x) any(is.na(x)) ))
# which classifiers contain missings
NA_classifiers <- which(apply(model_preds, 2, function(x) any(is.na(x))))
NA_classifiers
# readjust weights without that classifier
final_model_weights
# readjust weights without that classifier
final_model_weights[-NA_classifiers]
# readjust weights without that classifier
NA_adj_weights <- final_model_weights[-NA_classifiers] / sum(final_model_weights[-NA_classifiers])
NA_adj_weights
# remove columns with NAs
no_Na_preds <- model_preds[,apply(X = model_preds, MARGIN = 2, FUN = function(x){
all(!is.na(x))})]
NA_adj_weights
no_Na_preds
no_Na_preds
# remove columns with NAs
no_Na_preds <- model_preds[
apply(X = model_preds, MARGIN = 2, FUN = function(x){all(!is.na(x))}), # rows
apply(X = model_preds, MARGIN = 2, FUN = function(x){all(!is.na(x))}) # columns
]
no_Na_preds
# remove columns with NAs
no_Na_preds <- model_preds[
m_idx, # rows
apply(X = model_preds, MARGIN = 2, FUN = function(x){all(!is.na(x))}) # columns
]
no_Na_preds
# predictions for rows with NAs on at least 1 classifier
no_NA_avg <- as.numeric(no_Na_preds %*% final_model_weights)
no_Na_preds
final_model_weights
# predictions for rows with NAs on at least 1 classifier
no_NA_avg <- as.numeric(no_Na_preds %*% NA_adj_weights)
no_NA_avg
# weighted average
model_preds <- as.matrix(post.strat$predictions$Level2[,names(final_model_weights)])
w_avg <- as.numeric(model_preds %*% final_model_weights)
w_avg
# missing row indexes
m_idx <- which(is.na(w_avg))
m_idx
# replace predictions
w_avg[m_idx] <- no_NA_avg
# weighted average
model_preds <- as.matrix(post.strat$predictions$Level2[,names(final_model_weights)])
w_avg <- as.numeric(model_preds %*% final_model_weights)
# deal with missings
if (any(is.na(w_avg))){
# missing row indexes
m_idx <- which(is.na(w_avg))
# which classifiers contain missings
NA_classifiers <- which(apply(model_preds, 2, function(x) any(is.na(x))))
# readjust weights without that classifier
NA_adj_weights <- final_model_weights[-NA_classifiers] / sum(final_model_weights[-NA_classifiers])
# remove columns with NAs
no_Na_preds <- model_preds[
m_idx, # rows
apply(X = model_preds, MARGIN = 2, FUN = function(x){all(!is.na(x))}) # columns
]
# predictions for rows with NAs on at least 1 classifier
no_NA_avg <- as.numeric(no_Na_preds %*% NA_adj_weights)
# replace predictions
w_avg[m_idx] <- no_NA_avg
}
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
L2_preds
library(autoMrP)
devtools::load_all()
set.seed(seed)
ebma_out <- ebma(
ebma.fold = ebma_fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
pc.names = pc_names,
post.strat = ps_out,
n.draws = ebma.n.draws,
tol = ebma.tol,
best.subset.opt = best_subset_out,
pca.opt = pca_out,
lasso.opt = lasso_out,
gb.opt = gb_out,
svm.opt = svm_out,
verbose = verbose,
cores = cores
)
ebma_out
rm(list=ls())
rm(list=ls())
devtools::load_all()
DATA <- foreign::read.dta("./../../../Desktop/lucas data/Minaret.dta")
data.census <- data.frame(matrix(NA,2*4*6*26,7))
data.census[,1] <- kronecker(c(1:26),rep(1, 48))
colnames(data.census)[1] <- "cantonnr"
data.census[,2] <- rep(kronecker(c(1:6),rep(1, 8)),26)
colnames(data.census)[2] <- "educ"
data.census[,3] <- rep(rep(kronecker(c(1:4),c(1,1)), 6),26)
colnames(data.census)[3] <- "agegroup"
data.census[,4] <- rep(rep(c(1:2),24),26)
colnames(data.census)[4] <- "female"
# Level 2
colnames(data.census)[5] <- "german"
data.census[,5] <- 0
german.canton <- which(table(DATA$german,DATA$cantonnr)[2,]!=0)
for (i in 1:26){
if (i %in% german.canton){
data.census[data.census$cantonnr==i,5] <- 1
}
}
colnames(data.census)[6] <- "proportion"
for (i in 1:26){
numbers.census <- swissMrP::census$census48.MAZH2013[,i]
numbers.census <- numbers.census/sum(numbers.census)
a <- 1+ (i-1)*48
b <- i*48
data.census[a:b,6] <- numbers.census
}
colnames(data.census)[7] <- "region"
cr.table <- table(DATA$cantonnr,DATA$region)
cr.table[11,2] <- 1
cr.table <- cbind(cr.table,rep(0,26))
cr.table[21,7] <- 1
for (i in 1:26){
fetch.region <- which(cr.table[i,]!=0)
data.census[data.census[,1]==i,7] <- fetch.region
}
# Lucas survey ------------------------------------------------------------
vox.data <- foreign::read.dta("./../../../Desktop/lucas data/vox_data.dta", convert.factors=FALSE)
# Let's look at the 10th AHV revision
data.4440 <- vox.data[vox.data$AbstNrBFS==4700,]
cantonnr <- data.4440$cantonnr
female <- data.4440$sexe
educ <- data.4440$educ
agegroup <- data.4440$age_group
region <- data.4440$region
vote.ahv <- data.4440$abst_entscheid
# Level-2 Variables.
deutsch <- data.4440$german_share
kath <- data.4440$romkath_share
links <- data.4440$leftvote_share
svp <- data.4440$svpvote_share
# Create matrix
data.analysis.4440 <- data.frame(cbind(vote.ahv, cantonnr, region, female, educ, agegroup, deutsch, kath, links, svp))
# add information to census block
deutsch.canton <- which(table(data.analysis.4440$deutsch,data.analysis.4440$cantonnr)[2,]!=0)
data.census$deutsch <- NA
for (i in 1:26){
XXXX <- unique(data.analysis.4440$deutsch[data.analysis.4440$cantonnr==i])
data.census$deutsch[data.census$cantonnr==i] <- XXXX
}
data.census$kath <- NA
for (i in 1:26){
XXXX <- unique(data.analysis.4440$kath[data.analysis.4440$cantonnr==i])
data.census$kath[data.census$cantonnr==i] <- XXXX
}
data.census$links <- NA
for (i in 1:26){
XXXX <- unique(data.analysis.4440$links[data.analysis.4440$cantonnr==i])
data.census$links[data.census$cantonnr==i] <- XXXX
}
data.census$svp <- NA
for (i in 1:26){
XXXX <- unique(data.analysis.4440$svp[data.analysis.4440$cantonnr==i])
data.census$svp[data.census$cantonnr==i] <- XXXX
}
mrp_model.3 <- auto_MrP(
y = "vote.ahv",
L1.x = c("female", "educ", "agegroup"),
L2.x = c("deutsch","links", "kath", "svp"),
L2.unit = "cantonnr",
cores = 3,
#L2.reg = "region",
survey = data.analysis.4440, # survey data
census = data.census, # census information
bin.proportion = "proportion",
best.subset = TRUE,
lasso = TRUE,
pca = TRUE,
gb = TRUE,
svm = TRUE,
mrp = TRUE)
mrp_model.3

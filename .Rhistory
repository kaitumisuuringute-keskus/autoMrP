L1.x = L1.x,
L2.x = pc_names,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
pca_out <- NULL
}
# Classifier 4: GB
if (isTRUE(gb)) {
message("Starting gradient tree boosting tuning")
# Determine context-level covariates
if (is.null(gb.L2.x)) {
gb.L2.x <- L2.x
}
# Evaluate inclusion of L2.unit in GB
if (isTRUE(gb.L2.unit)) {
gb.L2.unit <- L2.unit
} else {
gb.L2.unit <- NULL
}
# Evaluate inclusion of L2.reg in GB
if (isTRUE(gb.L2.reg)) {
gb.L2.reg <- L2.reg
} else {
gb.L2.reg <- NULL
}
# Run classifier
gb_out <- run_gb(y = y,
L1.x = L1.x,
L2.x = gb.L2.x,
L2.unit = gb.L2.unit,
L2.reg = gb.L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
interaction.depth = gb.interaction.depth,
shrinkage = gb.shrinkage,
n.trees.init = gb.n.trees.init,
n.trees.increase = gb.n.trees.increase,
n.trees.max = gb.n.trees.max,
n.iter = gb.n.iter,
n.minobsinnode = gb.n.minobsinnode,
data = cv_folds,
verbose = verbose)
} else {
gb_out <- NULL
}
# Classifier 5: SVM
if (isTRUE(svm)) {
message("Starting support vector machine tuning")
# Determine context-level covariates
if (is.null(svm.L2.x)) {
svm.L2.x <- L2.x
}
# Evaluate inclusion of L2.unit in GB
if (isTRUE(svm.L2.unit)) {
svm.L2.unit <- L2.unit
} else {
svm.L2.unit <- NULL
}
# Evaluate inclusion of L2.reg in GB
if (isTRUE(svm.L2.reg)) {
svm.L2.reg <- L2.reg
} else {
svm.L2.reg <- NULL
}
# Run classifier
svm_out <- run_svm(y = y,
L1.x = L1.x,
L2.x = svm.L2.x,
L2.unit = svm.L2.unit,
L2.reg = svm.L2.reg,
kernel = svm.kernel,
loss.fun = loss.fun,
loss.unit = loss.unit,
gamma = svm.gamma,
cost = svm.cost,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
svm_out <- NULL
}
# --------------------------- Post-stratification ----------------------------
message("Starting post-stratification")
ps_out <- post_stratification(
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
best.subset.opt = best_subset_out,
lasso.opt = lasso_out,
pca.opt = pca_out,
gb.opt = gb_out,
svm.opt = svm_out,
mrp.include = mrp,
n.minobsinnode = gb.n.minobsinnode,
L2.unit.include = gb.L2.unit,
L2.reg.include = gb.L2.reg,
kernel = svm.kernel,
mrp.L2.x = mrp.L2.x,
data = cv_data,
ebma.fold = ebma_fold,
census = census,
verbose = verbose
)
# ----------------------------------- EBMA -----------------------------------
ebma_out <- ebma(
ebma.fold = ebma_fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
post.strat = ps_out,
n.draws = ebma.n.draws,
tol = ebma.tol,
best.subset.opt = best_subset_out,
pca.opt = pca_out,
lasso.opt = lasso_out,
gb.opt = gb_out,
svm.opt = svm_out,
verbose = verbose,
cores = cores
)
# ----------------------------- Function output ------------------------------
return(ebma_out)
}
auto_MrP <- function(y, L1.x, L2.x, L2.unit, L2.reg = NULL, L2.x.scale = TRUE,
pcs = NULL, folds = NULL, bin.proportion = NULL,
bin.size = NULL, survey, census, ebma.size = 1/3,
cores = 1, k.folds = 5, cv.sampling = "L2 units",
loss.unit = "individuals", loss.fun = "MSE",
best.subset = TRUE, lasso = TRUE, pca = TRUE, gb = TRUE,
svm = TRUE, mrp = FALSE, forward.select = FALSE,
best.subset.L2.x = NULL, lasso.L2.x = NULL,
pca.L2.x = NULL, gb.L2.x = NULL, svm.L2.x = NULL,
mrp.L2.x = NULL, gb.L2.unit = FALSE, gb.L2.reg = FALSE,
svm.L2.unit = FALSE, svm.L2.reg = FALSE,
lasso.lambda = 1 / exp(- seq(from = -1, to = 4.5, length = 100)),
lasso.n.iter = 70, gb.interaction.depth = c(1, 2, 3),
gb.shrinkage = c(0.04, 0.01, 0.008, 0.005, 0.001),
gb.n.trees.init = 50, gb.n.trees.increase = 50,
gb.n.trees.max = 1000, gb.n.iter = 70,
gb.n.minobsinnode = 5, svm.kernel = "radial",
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65,
0.7, 0.8, 0.9, 1, 2, 3, 4),
svm.cost = c(1, 10), ebma.n.draws = 100,
ebma.tol = c(0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005,
0.00001), uncertainty = FALSE, seed = NULL,
verbose = FALSE) {
# ----------------------------------- Seed -----------------------------------
# Check seed argument and set seed
if (is.null(seed)) {
set.seed(546213978)
} else {
if (isTRUE(dplyr::near(seed, as.integer(seed)))) {
set.seed(seed)
} else {
stop("Seed must be either NULL or an integer-valued scalar.")
}
}
# ------------------------------- Error checks -------------------------------
# Call to function doing the error checks
error_checks(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
L2.x.scale = L2.x.scale,
pcs = pcs,
folds = folds,
bin.proportion = bin.proportion,
bin.size = bin.size,
survey = survey,
census = census,
ebma.size = ebma.size,
k.folds = k.folds,
cv.sampling = cv.sampling,
loss.unit = loss.unit,
loss.fun = loss.fun,
best.subset = best.subset,
lasso = lasso,
pca = pca,
gb = gb,
svm = svm,
mrp = mrp,
forward.select = forward.select,
best.subset.L2.x = best.subset.L2.x,
lasso.L2.x = lasso.L2.x,
gb.L2.x = gb.L2.x,
svm.L2.x = svm.L2.x,
mrp.L2.x = mrp.L2.x,
gb.L2.unit = gb.L2.unit,
gb.L2.reg = gb.L2.reg,
lasso.lambda = lasso.lambda,
lasso.n.iter = lasso.n.iter)
#if (!(is.null(lasso.iterations.max) | (is.numeric(lasso.iterations.max) &
#                                       length(lasso.iterations.max) == 1))) {
#  stop("lasso.iterations.max must be either a numeric scalar or NULL.")
#}
#if (!(is.integer(gb.interaction.set) |
#      all(as.integer(gb.interaction.set) == gb.interaction.set))) {
#  stop("gb.interaction.set must be an integer-valued vector.")
#}
#if (!is.numeric(gb.shrinkage.set)) {
#  stop("gb.shrinkage.set must be a numeric vector")
#} else if (min(gb.shrinkage.set) < 0.001 | max(gb.shrinkage.set) > 0.1) {
#  warning("gb.shrinkage.set should have values lying between 0.001 and 0.1.")
#}
#if (!((is.integer(gb.tree.start) |
#       all(as.integer(gb.tree.start) == gb.tree.start)) &
#      length(gb.tree.start) == 1)) {
#  stop("gb.tree.start must be an integer-valued scalar.")
#}
#if (!(is.integer(gb.tree.increase.set) |
#      all(as.integer(gb.tree.increase.set) == gb.tree.increase.set))) {
#  stop("gb.tree.increase.set must be an integer-valued scalar or vector.")
#} else if (length(gb.tree.increase.set) > 1 &
#           length(gb.tree.increase.set) != length(gb.shrinkage.set)) {
#  stop(paste("gb.tree.increase.set must be either a scalar or a vector of ",
#             "size `length(gb.shrinkage.set)`.", sep = ""))
#}
#if (!(is.integer(gb.trees.max.set) |
#      all(as.integer(gb.trees.max.set) == gb.trees.max.set))) {
#  stop("gb.trees.max.set must be an integer-valued scalar or vector.")
#} else if (length(gb.trees.max.set) > 1 &
#           length(gb.trees.max.set) != length(gb.shrinkage.set)) {
#  stop(paste("gb.trees.max.set must be either a scalar or a vector of size ",
#             "`length(gb.shrinkage.set)`.", sep = ""))
#}
# ----------------------------- Parallel computing ----------------------------
if (!uncertainty){
# ------------------------------- Prepare data -------------------------------
# Coerce individual-level variables and geographic variables to factors in
# survey and census data
survey <- survey %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), .funs = as.factor)
census <- census %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), .funs = as.factor)
# If not provided in census data, calculate bin size and bin proportion for
# each ideal type in a geographic unit
if (is.null(bin.proportion)) {
if (is.null(bin.size)) {
census <- census %>%
dplyr::group_by(.dots = c(L1.x, L2.unit)) %>%
dplyr::summarise(n = dplyr::n())
} else {
census$n <- census[[bin.size]]
}
census <- census %>%
dplyr::group_by(.dots = L2.unit) %>%
dplyr::mutate(prop = n / sum(n))
} else {
census <- census %>%
dplyr::rename(prop = one_of(bin.proportion))
}
# If not provided in survey and census data, compute the principal components
# of context-level variables
if (is.null(pcs)) {
# Determine context-level covariates whose principal components are to be
# computed
if (is.null(pca.L2.x)) {
pca.L2.x <- L2.x
}
# Compute principal components for survey data
pca_out <- stats::prcomp(survey[, pca.L2.x],
retx = TRUE,
center = TRUE,
scale. = TRUE,
tol = NULL)
# Add PCs to survey data
survey <- survey %>%
dplyr::bind_cols(as.data.frame(pca_out$x))
# Add PCs to census data
pc_names <- colnames(pca_out$x)
census <- census %>%
dplyr::left_join(unique(survey %>% dplyr::select(all_of(L2.unit),
all_of(pc_names))),
by = L2.unit)
} else {
pc_names <- pcs
}
# Scale context-level variables in survey and census data
if (isTRUE(L2.x.scale)) {
survey[, L2.x] <- scale(survey[, L2.x], center = TRUE, scale = TRUE)
census[, L2.x] <- scale(census[, L2.x], center = TRUE, scale = TRUE)
}
# Convert survey and census data to tibble
survey <- tibble::as_tibble(x = survey)
census <- tibble::as_tibble(x = census)
# ------------------------------- Create folds -------------------------------
if (is.null(folds)) {
# EBMA hold-out fold
ebma.size <- round(nrow(survey) * ebma.size, digits = 0)
if(ebma.size>0){
ebma_folding_out <- ebma_folding(data = survey,
L2.unit = L2.unit,
ebma.size = ebma.size)
ebma_fold <- ebma_folding_out$ebma_fold
cv_data <- ebma_folding_out$cv_data
} else{
ebma_fold <- NULL
cv_data <- survey
}
# K folds for cross-validation
cv_folds <- cv_folding(data = cv_data,
L2.unit = L2.unit,
k.folds = k.folds,
cv.sampling = cv.sampling)
} else {
if (ebma.size > 0){
# EBMA hold-out fold
ebma_fold <- survey %>%
dplyr::filter_at(dplyr::vars(dplyr::one_of(folds)),
dplyr::any_vars(. == k.folds + 1))
}
# K folds for cross-validation
cv_data <- survey %>%
dplyr::filter_at(dplyr::vars(dplyr::one_of(folds)),
dplyr::any_vars(. != k.folds + 1))
cv_folds <- cv_data %>%
dplyr::group_split(.data[[folds]])
}
# ---------------------- Optimal individual classifiers ----------------------
# Classifier 1: Best Subset
if (isTRUE(best.subset)) {
message("Starting multilevel regression with best subset selection classifier tuning")
# Determine context-level covariates
if (is.null(best.subset.L2.x)) {
best.subset.L2.x <- L2.x
}
# Run classifier
best_subset_out <- run_best_subset(y = y,
L1.x = L1.x,
L2.x = best.subset.L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
best_subset_out <- NULL
}
# Classifier 2: Lasso
if (isTRUE(lasso)) {
message("Starting multilevel regression with L1 regularization tuning")
# Determine context-level covariates
if (is.null(lasso.L2.x)) {
lasso.L2.x <- L2.x
}
# Run classifier
lasso_out <- run_lasso(y = y,
L1.x = L1.x,
L2.x = lasso.L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
lambda = lasso.lambda,
n.iter = lasso.n.iter,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
lasso_out <- NULL
}
# Classifier 3: PCA
if (isTRUE(pca)) {
message("Starting multilevel regression with principal components as context level variables tuning")
pca_out <- run_pca(
y = y,
L1.x = L1.x,
L2.x = pc_names,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
pca_out <- NULL
}
# Classifier 4: GB
if (isTRUE(gb)) {
message("Starting gradient tree boosting tuning")
# Determine context-level covariates
if (is.null(gb.L2.x)) {
gb.L2.x <- L2.x
}
# Evaluate inclusion of L2.unit in GB
if (isTRUE(gb.L2.unit)) {
gb.L2.unit <- L2.unit
} else {
gb.L2.unit <- NULL
}
# Evaluate inclusion of L2.reg in GB
if (isTRUE(gb.L2.reg)) {
gb.L2.reg <- L2.reg
} else {
gb.L2.reg <- NULL
}
# Run classifier
gb_out <- run_gb(y = y,
L1.x = L1.x,
L2.x = gb.L2.x,
L2.unit = gb.L2.unit,
L2.reg = gb.L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
interaction.depth = gb.interaction.depth,
shrinkage = gb.shrinkage,
n.trees.init = gb.n.trees.init,
n.trees.increase = gb.n.trees.increase,
n.trees.max = gb.n.trees.max,
n.iter = gb.n.iter,
n.minobsinnode = gb.n.minobsinnode,
data = cv_folds,
verbose = verbose)
} else {
gb_out <- NULL
}
# Classifier 5: SVM
if (isTRUE(svm)) {
message("Starting support vector machine tuning")
# Determine context-level covariates
if (is.null(svm.L2.x)) {
svm.L2.x <- L2.x
}
# Evaluate inclusion of L2.unit in GB
if (isTRUE(svm.L2.unit)) {
svm.L2.unit <- L2.unit
} else {
svm.L2.unit <- NULL
}
# Evaluate inclusion of L2.reg in GB
if (isTRUE(svm.L2.reg)) {
svm.L2.reg <- L2.reg
} else {
svm.L2.reg <- NULL
}
# Run classifier
svm_out <- run_svm(y = y,
L1.x = L1.x,
L2.x = svm.L2.x,
L2.unit = svm.L2.unit,
L2.reg = svm.L2.reg,
kernel = svm.kernel,
loss.fun = loss.fun,
loss.unit = loss.unit,
gamma = svm.gamma,
cost = svm.cost,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
svm_out <- NULL
}
# --------------------------- Post-stratification ----------------------------
message("Starting post-stratification")
ps_out <- post_stratification(
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
best.subset.opt = best_subset_out,
lasso.opt = lasso_out,
pca.opt = pca_out,
gb.opt = gb_out,
svm.opt = svm_out,
mrp.include = mrp,
n.minobsinnode = gb.n.minobsinnode,
L2.unit.include = gb.L2.unit,
L2.reg.include = gb.L2.reg,
kernel = svm.kernel,
mrp.L2.x = mrp.L2.x,
data = cv_data,
ebma.fold = ebma_fold,
census = census,
verbose = verbose
)
# ----------------------------------- EBMA -----------------------------------
ebma_out <- ebma(
ebma.fold = ebma_fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
post.strat = ps_out,
n.draws = ebma.n.draws,
tol = ebma.tol,
best.subset.opt = best_subset_out,
pca.opt = pca_out,
lasso.opt = lasso_out,
gb.opt = gb_out,
svm.opt = svm_out,
verbose = verbose,
cores = cores
)
# ----------------------------- Function output ------------------------------
return(ebma_out)
}

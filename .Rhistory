mrp = TRUE
forward.select = FALSE
best.subset.L2.x = NULL
lasso.L2.x = NULL
pca.L2.x = NULL
gb.L2.x = NULL
svm.L2.x = NULL
# Standard MrP model L2 variables
gb.L2.unit = FALSE
gb.L2.reg = FALSE
# tuning params lasso
lasso.lambda = list(c(1), c(10))
lasso.n.iter = 20
# tuning params boosting
gb.interaction.depth = c(1, 2, 3)
gb.shrinkage = c(0.04, 0.01)
gb.n.trees.init = 50
gb.n.trees.increase = 50
gb.n.trees.max = 1000
gb.n.iter = 70
gb.n.minobsinnode = 5
svm.kernel = "radial"
svm.loss.fun = NULL
svm.gamma = c(0.3, 0.5)
svm.cost = c(1, 10)
ebma.n.draws = 3
ebma.tol = c(0.01, 0.005)
uncertainty = FALSE
seed = NULL
verbose = TRUE
# Determine context-level covariates
if (is.null(svm.L2.x)) {
svm.L2.x <- L2.x
}
svm.L2.x
L2.x = svm.L2.x
# Create model formula
x <- paste(c(L1.x, L2.x, L2.unit, L2.reg), collapse = " + ")
form <- as.formula(paste(y, " ~ ", x, sep = ""))
form
# tuning parameter grid
svm_grid <- expand.grid(gamma, cost)
names(svm_grid) <- c("gamma", "cost")
svm_grid
# loop over tuning grid
grid_cells <- apply(svm_grid, 1, function(g) {
# Set tuning parameters
gamma_value <- as.numeric(g["gamma"])
cost_value <- as.numeric(g["cost"])
# Loop over each fold
k_errors <- lapply(seq_along(data), function(k) {
# Split data in training and validation sets and factorize DV
data_train <- dplyr::bind_rows(data[-k]) %>%
dplyr::mutate_at(.vars = y, as.factor)
data_valid <- dplyr::bind_rows(data[k]) %>%
dplyr::mutate_at(.vars = y, as.factor)
# Svm classifier
model_l <- svm_classifier(
form = form,
data = data_train,
kernel = kernel,
type = "C-classification",
probability = TRUE,
svm.gamma = gamma_value,
svm.cost = cost_value,
verbose = verbose
)
# Use trained model to make predictions for kth validation set
pred_l <- predict(model_l, newdata = data.frame(data_valid),
probability = TRUE)
pred_l <- as.numeric(attr(pred_l, "probabilities")[, "1"])
# Transform factor DV to numeric for loss function
data_valid <- data_valid %>%
dplyr::mutate_at(.vars = y, function(x) as.numeric(levels(x))[x])
# Evaluate predictions based on loss function
perform_l <- loss_function(pred = pred_l, data.valid = data_valid,
loss.unit = loss.unit,
loss.fun = loss.fun,
y = y, L2.unit = L2.unit)
})
# Mean over all k folds
best_error <- mean(unlist(k_errors))
})
loss.unit
loss.fun
# Extract best tuning parameters
out <- list(gamma = svm_grid[which.min(grid_cells), "gamma"],
cost = svm_grid[which.min(grid_cells), "cost"])
out
# Create list of cross-validation folds
cv_folds <- list(
`1` = survey_item[1:200, ],
`2` = survey_item[201:400, ],
`3` = survey_item[401:1500, ])
# Run svm classifier
m <- run_svm(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2"),
L2.unit = "state",
L2.reg = "region",
kernel = "radial",
loss.fun = "MSE",
loss.unit = "individuals",
gamma = c(0.3, 0.1),
cost = c(1, 50),
data = cv_folds,
verbose = TRUE)
m
devtools::load_all()
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = TRUE,
lasso = TRUE,
pca = TRUE,
gb = TRUE,
svm = TRUE,
mrp = TRUE,
mrp.L2.x = c("L2.x1", "L2.x2"),
ebma.tol = c(0.001, 0.0005),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
run_svm <- function(y, L1.x, L2.x, L2.unit, L2.reg,
kernel = "radial", loss.fun,
loss.unit, gamma, cost, data,
verbose) {
# Create model formula
x <- paste(c(L1.x, L2.x, L2.unit, L2.reg), collapse = " + ")
form <- as.formula(paste(y, " ~ ", x, sep = ""))
# tuning parameter grid
svm_grid <- expand.grid(gamma, cost)
names(svm_grid) <- c("gamma", "cost")
# loop over tuning grid
grid_cells <- apply(svm_grid, 1, function(g) {
# Set tuning parameters
gamma_value <- as.numeric(g["gamma"])
cost_value <- as.numeric(g["cost"])
# Loop over each fold
k_errors <- lapply(seq_along(data), function(k) {
browser()
# Split data in training and validation sets and factorize DV
data_train <- dplyr::bind_rows(data[-k]) %>%
dplyr::mutate_at(.vars = y, as.factor)
data_valid <- dplyr::bind_rows(data[k]) %>%
dplyr::mutate_at(.vars = y, as.factor)
# Svm classifier
model_l <- svm_classifier(
form = form,
data = data_train,
kernel = kernel,
type = "C-classification",
probability = TRUE,
svm.gamma = gamma_value,
svm.cost = cost_value,
verbose = verbose
)
# Use trained model to make predictions for kth validation set
pred_l <- predict(model_l, newdata = data.frame(data_valid),
probability = TRUE)
pred_l <- as.numeric(attr(pred_l, "probabilities")[, "1"])
# Transform factor DV to numeric for loss function
data_valid <- data_valid %>%
dplyr::mutate_at(.vars = y, function(x) as.numeric(levels(x))[x])
# Evaluate predictions based on loss function
perform_l <- loss_function(pred = pred_l, data.valid = data_valid,
loss.unit = loss.unit,
loss.fun = loss.fun,
y = y, L2.unit = L2.unit)
})
# Mean over all k folds
best_error <- mean(unlist(k_errors))
})
# Extract best tuning parameters
out <- list(gamma = svm_grid[which.min(grid_cells), "gamma"],
cost = svm_grid[which.min(grid_cells), "cost"])
# Function output
return(out)
}
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = TRUE,
lasso = TRUE,
pca = TRUE,
gb = TRUE,
svm = TRUE,
mrp = TRUE,
mrp.L2.x = c("L2.x1", "L2.x2"),
ebma.tol = c(0.001, 0.0005),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = TRUE,
lasso = TRUE,
pca = TRUE,
gb = TRUE,
svm = TRUE,
mrp = TRUE,
mrp.L2.x = c("L2.x1", "L2.x2"),
ebma.tol = c(0.001, 0.0005),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
loss.unit
loss.fun
n
>>>>>>> d6c78e96e68f30a639f80a7e5f896379f3624347
library(autoMrP)
# load all package functions
devtools::load_all()
# data
survey <- autoMrP::survey_item
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = TRUE,
lasso = TRUE,
pca = TRUE,
gb = TRUE,
svm = TRUE,
mrp = TRUE,
mrp.L2.x = c("L2.x1", "L2.x2"),
ebma.tol = c(0.001, 0.0005),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
# data
survey <- autoMrP::survey_item
# load all package functions
devtools::load_all()
# test function
rm(list=ls())
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = TRUE,
lasso = TRUE,
pca = TRUE,
gb = TRUE,
svm = TRUE,
mrp = TRUE,
mrp.L2.x = c("L2.x1", "L2.x2"),
ebma.tol = c(0.001, 0.0005),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
out
library(autoMrP)
# load all package functions
devtools::load_all()
# test function
rm(list=ls())
# load all package functions
devtools::load_all()
# data
survey <- autoMrP::survey_item
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = TRUE,
lasso = TRUE,
pca = TRUE,
gb = TRUE,
svm = TRUE,
mrp = TRUE,
mrp.L2.x = c("L2.x1", "L2.x2"),
ebma.tol = c(0.001, 0.0005),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
out
library(autoMrP)
# test function
rm(list=ls())
# data
survey <- autoMrP::survey_item
# load all package functions
devtools::load_all()
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = TRUE,
lasso = TRUE,
pca = TRUE,
gb = TRUE,
svm = TRUE,
mrp = TRUE,
mrp.L2.x = c("L2.x1", "L2.x2"),
ebma.tol = c(0.001, 0.0005),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
out
?autoMrP
?autoMrP::autoMrP
autoMrP::autoMrP
# load all package functions
devtools::load_all()
?auto_MrP
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = FALSE,
lasso = FALSE,
pca = FALSE,
gb = FALSE,
svm = TRUE,
mrp = FALSE,
ebma.tol = c(0.001, 0.0005),
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
svm.cost = c(1, 10),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
out
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = FALSE,
lasso = FALSE,
pca = FALSE,
gb = FALSE,
svm = TRUE,
mrp = FALSE,
ebma.tol = c(0.001, 0.0005),
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
svm.cost = c(0.5, 1, 10),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
out
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = FALSE,
lasso = FALSE,
pca = FALSE,
gb = FALSE,
svm = TRUE,
mrp = FALSE,
ebma.tol = c(0.001, 0.0005),
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
svm.cost = c(0.1, 0.5, 1, 5, 10, 20),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
out
print(out$classifiers, n  = 48)
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = FALSE,
lasso = FALSE,
pca = FALSE,
gb = TRUE,
svm = TRUE,
mrp = FALSE,
ebma.tol = c(0.001, 0.0005),
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
svm.cost = c(0.1, 0.5, 1, 5, 10, 20, 50, 100),
ebma.n.draws = 2,
gb.L2.unit = FALSE,
lasso.n.iter = 5
)
out
library(autoMrP)
library(autoMrP)
<<<<<<< HEAD
devtools::install_github("retowuest/autoMrP",
auth_token = "042a7ceb4aeca462c4479d54ad40b5926563c4a6",
force = TRUE)
?autoMrP::auto_MrP
install.packages("knitr")
library(knitr)
install.packages("knitr", lib="C:/Users/Philipp/Dropbox/MRP and ML/autoMrP/packrat/lib-R/x86_64-w64-mingw32/4.0.0")
devtools::install_github("retowuest/autoMrP",lib="C:/Users/Philipp/Dropbox/MRP and ML/autoMrP/packrat/lib-R/x86_64-w64-mingw32/4.0.0")
install.packages("devtools", lib="C:/Users/Philipp/Dropbox/MRP and ML/autoMrP/packrat/lib-R/x86_64-w64-mingw32/4.0.0")
devtools::install_github("retowuest/autoMrP",lib="C:/Users/Philipp/Dropbox/MRP and ML/autoMrP/packrat/lib-R/x86_64-w64-mingw32/4.0.0")
devtools::install_github("cran/EBMAforecast",lib="C:/Users/Philipp/Dropbox/MRP and ML/autoMrP/packrat/lib-R/x86_64-w64-mingw32/4.0.0")
library(autoMrP)
?auto_MrP::census
?autoMrP::census
library(autoMrP)
=======
# test function
rm(list=ls())
# data
survey <- autoMrP::survey_item
# load all package functions
devtools::load_all()
out <- autoMrP::auto_MrP(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = FALSE,
lasso = FALSE,
pca = FALSE,
gb = TRUE,
svm = TRUE,
mrp = FALSE,
ebma.tol = c(0.001, 0.0005),
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
svm.cost = c(0.1, 0.5, 1, 5, 10, 20, 50, 100),
svm.L2.unit = FALSE,
svm.L2.reg = FALSE,
ebma.n.draws = 2,
gb.L2.reg = TRUE,
lasso.n.iter = 5
)
out
print(out$classifiers, n = 48)
>>>>>>> d6c78e96e68f30a639f80a7e5f896379f3624347
library(autoMrP)
library(autoMrP)

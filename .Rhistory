c()
head(pred_l)
perform_l <- loss_function(pred = pred_l, data.valid = data_valid,
unit = "individual", measure = "mse",
y = y, geo.unit = geo.unit)
class(perform_l)
length(perform_l)
head(perform_l)
perform_l <- loss_function(pred = pred_l, data.valid = data_valid,
unit = "individual", measure = "mse",
y = y, geo.unit = geo.unit)
length(perform_l)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data_valid)
perform_l <- loss_function(pred = pred_l, data.valid = data_valid,
unit = "individual", measure = "mse",
y = y, geo.unit = geo.unit)
perform_l
k_errors <- lapply(seq_along(cv.data), function(k) {
# Split data in training and validation sets
data_train <- dplyr::bind_rows(cv.data[-k])
data_valid <- dplyr::bind_rows(cv.data[k])
# Train model using lambda on kth training set
model_l <- lasso_classifier(L2.fix = L2_fe_form,
L1.re = L1_re,
data.train = data_train,
lambda = lambda,
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data_valid)
# Evaluate predictions based on loss function
perform_l <- loss_function(pred = pred_l, data.valid = data_valid,
unit = "individual", measure = "mse",
y = y, geo.unit = geo.unit)
})
perform_l
k_errors
lambda.set
lambda
lambda.set <- c(1, 2)
lambda_errors <- lapply(seq_along(lambda.set), function(lambda) {
# Print lambda
if (verbose == TRUE) {
L <- length(lambda.set)
cat(paste("Lasso: Running lambda ", lambda,
" out of ", L, " lambdas\n", sep = ""))
}
# Loop over each fold
k_errors <- lapply(seq_along(cv.data), function(k) {
# Split data in training and validation sets
data_train <- dplyr::bind_rows(cv.data[-k])
data_valid <- dplyr::bind_rows(cv.data[k])
# Train model using lambda on kth training set
model_l <- lasso_classifier(L2.fix = L2_fe_form,
L1.re = L1_re,
data.train = data_train,
lambda = lambda,
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data_valid)
# Evaluate predictions based on loss function
perform_l <- loss_function(pred = pred_l, data.valid = data_valid,
unit = "individual", measure = "mse",
y = y, geo.unit = geo.unit)
})
# Mean over all k folds
mean(unlist(k_errors))
})
lambda_errors
m_errors
lambda.set
lambda_errors <- lapply(seq_along(lambda.set), function(lambda) {
# Print lambda
if (verbose == TRUE) {
L <- length(lambda.set)
cat(paste("Lasso: Running lambda ", lambda,
" out of ", L, " lambdas\n", sep = ""))
}
# Loop over each fold
k_errors <- lapply(seq_along(cv.data), function(k) {
# Split data in training and validation sets
data_train <- dplyr::bind_rows(cv.data[-k])
data_valid <- dplyr::bind_rows(cv.data[k])
# Train model using lambda on kth training set
model_l <- lasso_classifier(L2.fix = L2_fe_form,
L1.re = L1_re,
data.train = data_train,
lambda = lambda,
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data_valid)
# Evaluate predictions based on loss function
perform_l <- loss_function(pred = pred_l, data.valid = data_valid,
unit = "individual", measure = "mse",
y = y, geo.unit = geo.unit)
})
# Mean over all k folds
mean(unlist(k_errors))
})
lambda_errors
# Choose best-performing model
min.lambda <- which.min(lambda_errors)
min.lambda
lambda.set
out <- lambda.set[min.lambda]
out
lambda.set <- data.frame(step_size = c(0.1, 0.3, 1, 10), cond_lambda = c(1, 10, NA, NA))
lambda.set
lambda.set <- data.frame(step_size = c(0.1, 0.3, 1, 10), lambda_lb = c(0, 1, 10, 100), lambda_ub = c(1, 10, NA, NA), n_iter = c(NA, NA, NA, 45))
lambda_set
lambda.set
max_iter
max_iter <- 10000
max_iter
seq_along(cv.data)
k
data_train <- dplyr::bind_rows(cv.data[-k])
data_valid <- dplyr::bind_rows(cv.data[k])
dim(data_train)
dim(data_valid)
L2_fe_form
L1_re
lambda
# Set initial lambda value to zero
lambda <- 0
lambda
verbose
model_init <- lasso_classifier(L2.fix = L2_fe_form,
L1.re = L1_re,
data.train = data_train,
lambda = lambda,
model.family = binomial(link = "probit"),
verbose = verbose)
summary(model_init)
# Use trained model to make predictions for kth validation set
pred_init <- stats::predict(model_init, newdata = data_valid)
length(pred_init)
dim(pred_init)
class(pred_init)
perform_init <- loss_function(pred = pred_init, data.valid = data_valid,
unit = "individual", measure = "mse",
y = y, geo.unit = geo.unit)
dim(perform_init)
class(perform_init)
length(perform_init)
k_errors_init <- lapply(seq_along(cv.data), function(k) {
# Split data in training and validation sets
data_train <- dplyr::bind_rows(cv.data[-k])
data_valid <- dplyr::bind_rows(cv.data[k])
# Train model using initial lambda on kth training set
model_init <- lasso_classifier(L2.fix = L2_fe_form,
L1.re = L1_re,
data.train = data_train,
lambda = lambda,
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_init <- stats::predict(model_init, newdata = data_valid)
# Evaluate predictions based on loss function
perform_init <- loss_function(pred = pred_init, data.valid = data_valid,
unit = "individual", measure = "mse",
y = y, geo.unit = geo.unit)
})
k_errors_init
lambda.set
lambda
lambda.set$step_size
max_iter
max_lambda
max_iter
max_iter
max_lambda <- 10000
max_iter <- 60
max.lambda <- 10000
max.iter <- 60
max.iter
lambda
# Initialize number of iterations
iter <- 0
iter
max.iter
iter
max.iter
verbose
iter
lambda
iter
max.iter
seq_along(cv.data)
k
k
data_train <- dplyr::bind_rows(cv.data[-k])
data_valid <- dplyr::bind_rows(cv.data[k])
lambda
model_l <- lasso_classifier(L2.fix = L2_fe_form,
L1.re = L1_re,
data.train = data_train,
lambda = lambda,
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data_valid)
perform_l <- loss_function(pred = pred_l, data.valid = data_valid,
unit = "individual", measure = "mse",
y = y, geo.unit = geo.unit)
perform_l
k_errors <- lapply(seq_along(cv.data), function(k) {
# Split data in training and validation sets
data_train <- dplyr::bind_rows(cv.data[-k])
data_valid <- dplyr::bind_rows(cv.data[k])
# Train model using lambda on kth training set
model_l <- lasso_classifier(L2.fix = L2_fe_form,
L1.re = L1_re,
data.train = data_train,
lambda = lambda,
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data_valid)
# Evaluate predictions based on loss function
perform_l <- loss_function(pred = pred_l, data.valid = data_valid,
unit = "individual", measure = "mse",
y = y, geo.unit = geo.unit)
})
k_errors
k_errors
# Mean over all k folds
mean(unlist(k_errors))
Inf
100000 < Inf
lambda
lambda.set
seq_along(lambda.set$step_size)
i
i <- 1
lambda.set$lambda_lb[i]
lambda.set <- data.frame(step_size = c(0.1, 0.3, 1, 10), lambda_lb = c(0, 1, 10, 100), lambda_ub = c(1, 10, NA, NA), n_iter = c(NA, NA, NA, 45))
lambda.set
lambda.set <- data.frame(step_size = c(0.1, 0.3, 1, 10), lambda_lb = c(0, 1, 10, 100), lambda_ub = c(1, 10, NA, NA))
lambda.set
lambda.set %>% dplyr::mutate_if(is.na, 0)
lambda.set %>% dplyr::mutate_if(NA, 0)
lambda.set %>% dplyr::mutate_if(is.na(), 0)
lambda.set$lambda_ub %>% dplyr::mutate_if(is.na(), 0)
lambda.set$lambda_ub %>% dplyr::mutate_if(is.na, 0)
lambda.set[, 3]
lambda.set[is.na(lambda.set[, 3]), 3]
max.iter
lambda.set
max.iter
max.lambda
# Replace NAs in upper limit for lambda with maximum number of iterations
lambda.set[is.na(lambda.set[, 3]), 3] <- max.lambda
lambda.set
seq_along(lambda.set[, 1])
i
lambda
lambda.set[i, 2]
lambda.set[i, 3]
lambda.set[i, 1]
i
if (lambda >= lambda.set[i, 2] &
lambda < lambda.set[i, 3]) {
lambda <- lambda + lambda.set[i, 1]
}
lambda
if (lambda >= lambda.set[i, 2] &
lambda < lambda.set[i, 3]) {
lambda <- lambda + lambda.set[i, 1]
}
lambda
for (i in seq_along(lambda.set[, 1])) {
if (lambda >= lambda.set[i, 2] &
lambda < lambda.set[i, 3]) {
lambda <- lambda + lambda.set[i, 1]
break
}
}
lambda
for (i in seq_along(lambda.set[, 1])) {
if (lambda >= lambda.set[i, 2] &
lambda < lambda.set[i, 3]) {
lambda <- lambda + lambda.set[i, 1]
break
}
}
lambda
lambda.set
lamdba <- 4
lambda
rm()
rm(lamdba)
lambda
lambda <- 4
for (i in seq_along(lambda.set[, 1])) {
if (lambda >= lambda.set[i, 2] &
lambda < lambda.set[i, 3]) {
lambda <- lambda + lambda.set[i, 1]
break
}
}
lambda
lambda <- 22
for (i in seq_along(lambda.set[, 1])) {
if (lambda >= lambda.set[i, 2] &
lambda < lambda.set[i, 3]) {
lambda <- lambda + lambda.set[i, 1]
break
}
}
lambda
lambda <- 500
for (i in seq_along(lambda.set[, 1])) {
if (lambda >= lambda.set[i, 2] &
lambda < lambda.set[i, 3]) {
lambda <- lambda + lambda.set[i, 1]
break
}
}
lambda
lambda
for (i in seq_along(lambda.set[, 1])) {
if (lambda >= lambda.set[i, 2] &
lambda < lambda.set[i, 3]) {
lambda <- lambda + lambda.set[i, 1]
break
}
}
lambda
lambda.set
seq_along(c(2, 3, 4))
seq_len(c(2, 3, 4))
seq_along(c(2, 3, 4))
rev(seq_along(c(2, 3, 4)))
rev(seq_along(lambda.set[, 1]))
lambda
for (i in rev(seq_along(lambda.set[, 1]))) {
if (lambda >= lambda.set[i, 2] &
lambda < lambda.set[i, 3]) {
lambda <- lambda + lambda.set[i, 1]
break
}
}
lambda
lambda.set
# Load packages
library(magrittr)
# Clean working environment
rm(list = ls())
# Load survey data
load(here::here("data", "survey_sample.RData"))
# Load census data
load(here::here("data", "census_data.RData"))
# Assign data sets to objects with "correct" names
survey <- survey_sample
census <- census_data
dim(survey)
head(survey)
# Define individual-level covariates
L1.x <- c("age", "educ", "gXr")
# Define context-level covariates
L2.x <- c("pvote", "religcon", "urban", "unemp", "hispanics", "white")
# Define context-level unit
L2.unit <- "stateid"
# Define region variable which groups context-level units
L2.re <- "region"
# Define outcome variable
y <- "y"
# Define column in census containing bin size
bin.size <- "n"
# Define fraction of EBMA size (NULL defaults to 1/4)
ebma.size <- NULL
# Define number of folds
k.folds <- 5
# Unit to be used in sampling to create CV folds
cv.sampling <- "units"
# Set seed (NULL defaults to 12345)
seed <- NULL
# Set seed
if (is.null(seed)) {
set.seed(12345)
} else {
set.seed(seed)
}
y
L1.x
L2.x
L2.unit
L2.re
survey
head(survey)
head(census)
bin.size
ebma.size
k.folds
cv.sampling
seed
# Set seed
if (is.null(seed)) {
set.seed(12345)
} else {
set.seed(seed)
}
if (!all(L1.x %in% colnames(survey))) {
stop(paste("Individual-level variable(s) '",
L1.x[which(!(L1.x %in% colnames(survey)))],
"' is/are not in your survey data.", sep = ""))
}
if (!all(L1.x %in% colnames(census))) {
stop(paste("Individual-level variable(s) '",
L1.x[which(!(L1.x %in% colnames(census)))],
"' is/are not in your census data.", sep = ""))
}
if (!all(L2.x %in% colnames(survey))) {
stop(paste("Context-level variable(s) '",
L2.x[which(!(L2.x %in% colnames(survey)))],
"' is/are not in your survey data.", sep = ""))
}
if (!all(L2.x %in% colnames(census))) {
stop(paste("Context-level variable(s) '",
L2.x[which(!(L2.x %in% colnames(census)))],
"' is/are not in your census data.", sep = ""))
}
if (!(y %in% colnames(survey))) {
stop(paste("Outcome '", y,
"' is not in your survey data.", sep = ""))
}
if (!(L2.unit %in% colnames(survey))) {
stop(paste("The geographic unit '", L2.unit,
"' is not in your survey data.", sep = ""))
}
if (!(L2.unit %in% colnames(census))) {
stop(paste("The geographic unit '", L2.unit,
"' is not in your census data.", sep = ""))
}
table(survey$stateid, survey$region)
head(survey[[L2.unit]])
head(survey[[c(L2.unit, L2.re)]])
head(survey[[L2.unit, L2.re]])
head(survey[c(L2.unit, L2.re)])
head(survey[, c(L2.unit, L2.re)])
head(unique(survey[, c(L2.unit, L2.re)]))
dim(unique(survey[, c(L2.unit, L2.re)]))
table(unique(survey[, c(L2.unit, L2.re)]))
rowSums(unique(survey[, c(L2.unit, L2.re)]))
dim(table(unique(survey[, c(L2.unit, L2.re)])))
apply(table(unique(survey[, c(L2.unit, L2.re)])), 1, function(x) x %in% c(0, 1))
apply(table(unique(survey[, c(L2.unit, L2.re)])), 1, function(x) sum(x) == 1)
all(apply(table(unique(survey[, c(L2.unit, L2.re)])), 1, function(x) sum(x) == 1))
any(apply(table(unique(survey[, c(L2.unit, L2.re)])), 1, function(x) sum(x) > 1))
which(apply(table(unique(survey[, c(L2.unit, L2.re)])), 1, function(x) sum(x) > 1))
rownames(table(unique(survey[, c(L2.unit, L2.re)])))
if (any(apply(table(unique(survey[, c(L2.unit, L2.re)])), 1,
function(x) sum(x) > 1))) {
stop(paste("The geographic unit(s) '",
rownames(table(unique(survey[, c(L2.unit, L2.re)])))[which(apply(table(unique(survey[, c(L2.unit, L2.re)])), 1,
function(x) sum(x) > 1))],
"' is/are nested in multiple regions."))
}
if (is.null(ebma.size)) {
ebma.size <- round(nrow(survey) / 4, digits = 0)
} else if (is.numeric(ebma.size) & ebma.size > 0 & ebma.size < 1) {
ebma.size <- round(nrow(survey) * ebma.size, digits = 0)
} else {
stop("ebma.size must be a rational number in the open unit interval.")
}
if (!(is.numeric(k.folds) & k.folds == round(k.folds, digits = 0))) {
stop("k.folds must be an integer number.")
}
if (!cv.sampling %in% c("respondents", "units")) {
stop("cv.sampling must take either the value 'respondents' or 'units'.")
}
bin.size
# If not provided in census data, calculate bin size for each ideal type
if (bin.size == "None") {
census <- census %>%
dplyr::group_by(.dots = L1.x) %>%
dplyr::summarise(n = dplyr::n())
} else {
census$n <- census[[bin.size]]
}
# Scale context-level variables in survey and census data
survey[, L2.x] <- scale(survey[, L2.x], center = TRUE, scale = TRUE)
census[, L2.x] <- scale(census[, L2.x], center = TRUE, scale = TRUE)
data = survey
L2.unit = L2.unit
ebma.size = ebma.size
dim(data)
head(data)
data <- data %>%
dplyr::mutate(index = row_number())
head(data)
dplyr::group_split(survey, .survey[[L2.unit]])
dplyr::group_split(survey, .data[[L2.unit]])
lapply(dplyr::group_split(survey, .data[[L2.unit]]), function(x) unique(x$region))
lapply(dplyr::group_split(survey, .data[[L2.unit]]), function(x) length(unique(x$region)) > 1)
any(lapply(dplyr::group_split(survey, .data[[L2.unit]]), function(x) length(unique(x$region)) > 1))
lapply(dplyr::group_split(survey, .data[[L2.unit]]), function(x) length(unique(x$region))))
lapply(dplyr::group_split(survey, .data[[L2.unit]]), function(x) length(unique(x$region)))
?lapply
lapply(dplyr::group_split(survey, .data[[L2.unit]]), function(x) length(unique(x$region)), simplify = TRUE)
unlist(lapply(dplyr::group_split(survey, .data[[L2.unit]]), function(x) length(unique(x$region))))
any(unlist(lapply(dplyr::group_split(survey, .data[[L2.unit]]), function(x) length(unique(x$region)))) > 1)
L2.re
any(unlist(lapply(dplyr::group_split(survey, .data[[L2.unit]]), function(x) length(unique(x[[L2.re]])))) > 1)
which(unlist(lapply(dplyr::group_split(survey, .data[[L2.unit]]), function(x) length(unique(x[[L2.re]])))) > 1)
data = survey
L2.unit = L2.unit
ebma.size = ebma.size

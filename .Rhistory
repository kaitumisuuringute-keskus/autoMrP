shrinkage = tuning_grid$s[min_e],
n_trees = out_d[[tuning_grid$d[min_e]]][[tuning_grid$s[min_e]]]$n_trees)
# Function output
return(out)
}
y = y
L1.x = L1.x
L2.x = L2.x
L2.unit = L2.unit
L2.reg = L2.reg
L2.unit.include = gb.L2.unit.include
L2.reg.include = gb.L2.reg.include
loss.unit = loss.unit
loss.measure = loss.measure
interaction.set = gb.interaction.set
shrinkage.set = gb.shrinkage.set
tree.start = gb.tree.start
tree.increase.set = gb.tree.increase.set
trees.max.set = gb.trees.max.set
iterations.max = gb.iterations.max
n.minobsinnode = gb.n.minobsinnode
data = cv_folds
verbose = verbose
y
L1.x
L2.x
L2.unit
L2.reg
L2.unit.include
L2.reg.include
loss.unit
loss.measure
interaction.set
shrinkage.set
gb.shrinkage.set <- gb.shrinkage.set[1:2]
shrinkage.set = gb.shrinkage.set
interaction.set
shrinkage.set
tree.start
tree.increase.set
gb.tree.increase.set <- c(50, 100)
tree.increase.set = gb.tree.increase.set
trees.max.set
gb.trees.max.set <- c(200, 400)
trees.max.set = gb.trees.max.set
iterations.max
n.minobsinnode
verbose
interaction.set
shrinkage.set
tree.start
tree.increase.set
trees.max.set
iterations.max
gb_out <- gb(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
L2.unit.include = gb.L2.unit.include,
L2.reg.include = gb.L2.reg.include,
loss.unit = loss.unit,
loss.measure = loss.measure,
interaction.set = gb.interaction.set,
shrinkage.set = gb.shrinkage.set,
tree.start = gb.tree.start,
tree.increase.set = gb.tree.increase.set,
trees.max.set = gb.trees.max.set,
iterations.max = gb.iterations.max,
n.minobsinnode = gb.n.minobsinnode,
data = cv_folds,
verbose = verbose)
gb_out
# Define initial number of total trees
gb.tree.start <- 50
# Define increase in number of trees as scalar for GB
gb.tree.increase.set <- 50
# Define maximum number of trees as scalar for GB
gb.trees.max.set <- 200
y = y
L1.x = L1.x
L2.x = L2.x
L2.unit = L2.unit
L2.reg = L2.reg
L2.unit.include = gb.L2.unit.include
L2.reg.include = gb.L2.reg.include
loss.unit = loss.unit
loss.measure = loss.measure
interaction.set = gb.interaction.set
shrinkage.set = gb.shrinkage.set
tree.start = gb.tree.start
tree.increase.set = gb.tree.increase.set
trees.max.set = gb.trees.max.set
iterations.max = gb.iterations.max
n.minobsinnode = gb.n.minobsinnode
data = cv_folds
verbose = verbose
y
L1.x
L2.x
L2.unit
L2.reg
L2.unit.include
L2.reg.include
loss.unit
loss.measure
interaction.set
shrinkage.set
tree.start
tree.increase.set
trees.max.set
iterations.max
n.minobsinnode
verbose
gb_out <- gb(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
L2.unit.include = gb.L2.unit.include,
L2.reg.include = gb.L2.reg.include,
loss.unit = loss.unit,
loss.measure = loss.measure,
interaction.set = gb.interaction.set,
shrinkage.set = gb.shrinkage.set,
tree.start = gb.tree.start,
tree.increase.set = gb.tree.increase.set,
trees.max.set = gb.trees.max.set,
iterations.max = gb.iterations.max,
n.minobsinnode = gb.n.minobsinnode,
data = cv_folds,
verbose = verbose)
gb_out
1 / (1 + exp(-0.1))
1 / (1 + exp(-0.8))
1 / (1 + exp(0.8))
?e1071::tune
install.packages(e1071)
install.packages("e1071")
?e1071::tune
L1.x
L2.x
L2.unit
# Define context-level unit
L2.unit <- "stateid"
# Define region in which context-level units are nested
L2.reg <- "region"
L1.x
L2.x
L2.unit
L2.reg
paste(c(L1.x, L2.x, L2.unit, L2.reg), collapse = " + ")
x <- paste(c(L1.x, L2.x, L2.unit, L2.reg), collapse = " + ")
form <- as.formula(paste(y, " ~ ", x, sep = ""))
form
y = y
L1.x = L1.x
L2.x = L2.x
L2.unit = L2.unit
L2.reg = L2.reg
kernel = svm.kernel
# Define kernel for SVM
svm.kernel <- "radial"
# Define error function for SVM
svm.error.fun <- "MSE"   # // might need to be changed to NULL
# Define gamma parameters for SVM
svm.gamma.set <- c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4)
# Define cost parameters for SVM
svm.cost.set <- c(1, 10)
svm_classifier <- function(method, form, data, kernel,
error.fun, probability,
gamma.set, cost.set,
sampling = "cross", cross,
verbose = c(TRUE, FALSE)) {
# Train and evaluate model using the supplied set of tuning parameters
if (isTRUE(verbose == TRUE)) {
out <- e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
tunecontrol = tune.control(sampling = sampling,
cross = cross))
} else {
out <- suppressMessages(suppressWarnings(
e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
tunecontrol = tune.control(sampling = sampling,
cross = cross))
))
}
}
y = y
L1.x = L1.x
L2.x = L2.x
L2.unit = L2.unit
L2.reg = L2.reg
kernel = svm.kernel
error.fun = svm.error.fun
gamma.set = svm.gamma.set
cost.set = svm.cost.set
k.folds = k.folds
data = cv_folds
verbose = verbose
y
L1.x
L2.x
L2.unit
L2.reg
kernel
error.fun
gamma.set
cost.set
k.folds
length(data)
verbose
x <- paste(c(L1.x, L2.x, L2.unit, L2.reg), collapse = " + ")
form <- as.formula(paste(y, " ~ ", x, sep = ""))
x
form
class(data)
data <- dplyr::bind_rows(data) %>%
dplyr::mutate_at(y = as.factor(yes))
y
data <- dplyr::bind_rows(data) %>%
dplyr::mutate_at(.vars = y, as.factor)
class(data)
dim(data)
form
kernel
error.fun
gamma.set
cost.set
k.folds
models <- svm_classifier(method = svm,
form = form,
data = data,
kernel = kernel,
error.fun = error.fun,
probability = TRUE,
gamma.set = gamma.set,
cost.set = cost.set,
sampling = "cross",
cross = k.folds,
verbose = TRUE)
svm_classifier <- function(method, form, data, kernel,
error.fun, probability,
gamma.set, cost.set,
sampling = "cross", cross,
verbose = c(TRUE, FALSE)) {
# Train and evaluate model using the supplied set of tuning parameters
if (isTRUE(verbose == TRUE)) {
out <- e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
e1071::tunecontrol = tune.control(sampling = sampling,
cross = cross))
} else {
out <- suppressMessages(suppressWarnings(
e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
e1071::tunecontrol = tune.control(sampling = sampling,
cross = cross))
))
}
}
svm_classifier <- function(method, form, data, kernel,
error.fun, probability,
gamma.set, cost.set,
sampling = "cross", cross,
verbose = c(TRUE, FALSE)) {
# Train and evaluate model using the supplied set of tuning parameters
if (isTRUE(verbose == TRUE)) {
out <- e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
e1071::tunecontrol = tune.control(sampling = sampling,
cross = cross))
} else {
out <- suppressMessages(suppressWarnings(
e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
e1071::tunecontrol = tune.control(sampling = sampling,
cross = cross))
))
}
# Function output
return(out)
}
out <- e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
e1071::tunecontrol = tune.control(sampling = sampling,
cross = cross))
?e1071::tunecontrol
svm_classifier <- function(method, form, data, kernel,
error.fun, probability,
gamma.set, cost.set,
sampling = "cross", cross,
verbose = c(TRUE, FALSE)) {
# Train and evaluate model using the supplied set of tuning parameters
if (isTRUE(verbose == TRUE)) {
out <- e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
tunecontrol = e1071::tune.control(sampling = sampling,
cross = cross))
} else {
out <- suppressMessages(suppressWarnings(
e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
tunecontrol = e1071::tune.control(sampling = sampling,
cross = cross))
))
}
# Function output
return(out)
}
models <- svm_classifier(method = svm,
form = form,
data = data,
kernel = kernel,
error.fun = error.fun,
probability = TRUE,
gamma.set = gamma.set,
cost.set = cost.set,
sampling = "cross",
cross = k.folds,
verbose = TRUE)
models <- svm_classifier(svm,
form = form,
data = data,
kernel = kernel,
error.fun = error.fun,
probability = TRUE,
gamma.set = gamma.set,
cost.set = cost.set,
sampling = "cross",
cross = k.folds,
verbose = TRUE)
models <- svm_classifier(method = "svm",
form = form,
data = data,
kernel = kernel,
error.fun = error.fun,
probability = TRUE,
gamma.set = gamma.set,
cost.set = cost.set,
sampling = "cross",
cross = k.folds,
verbose = TRUE)
models
svm_classifier <- function(method, form, data, kernel,
error.fun, probability,
gamma.set, cost.set,
sampling = "cross", cross,
verbose = c(TRUE, FALSE)) {
# Train and evaluate model using the supplied set of tuning parameters
if (isTRUE(verbose == TRUE)) {
out <- e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
tunecontrol = e1071::tune.control(sampling = sampling,
cross = cross))
} else {
out <- suppressMessages(suppressWarnings(
e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
tunecontrol = e1071::tune.control(sampling = sampling,
cross = cross))
))
}
# Function output
return(out)
}
# Extract the best model
best_model <- models$best.model
best_model
class(best_model)
best_model$type
best_model$kernel
best_model$cost
best_model$gamma
# Extract the best model
best_model <- models$best.model
out <- list(gamma = best_model$gamma,
cost = best_model$cost)
out
svm_out <- svm(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
kernel = svm.kernel,
error.fun = svm.error.fun,
gamma.set = svm.gamma.set,
cost.set = svm.cost.set,
k.folds = k.folds,
data = cv_folds,
verbose = verbose)
svm_classifier <- function(method, form, data, kernel,
error.fun, probability,
gamma.set, cost.set,
sampling = "cross", cross,
verbose = c(TRUE, FALSE)) {
# Train and evaluate model using the supplied set of tuning parameters
if (isTRUE(verbose == TRUE)) {
out <- e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
tunecontrol = e1071::tune.control(sampling = sampling,
cross = cross))
} else {
out <- suppressMessages(suppressWarnings(
e1071::tune(method = method, train.x = form,
data = data, kernel = kernel,
error.fun = error.fun,
probability = probability,
ranges = list(gamma = gamma.set,
cost = cost.set),
tunecontrol = e1071::tune.control(sampling = sampling,
cross = cross))
))
}
# Function output
return(out)
}
svm <- function(y, L1.x, L2.x, L2.unit, L2.reg,
kernel = "radial", error.fun,
gamma.set, cost.set, k.folds,
data, verbose) {
# Create model formula
x <- paste(c(L1.x, L2.x, L2.unit, L2.reg), collapse = " + ")
form <- as.formula(paste(y, " ~ ", x, sep = ""))
# Prepare data
data <- dplyr::bind_rows(data) %>%
dplyr::mutate_at(.vars = y, as.factor)
# Train and evaluate model using the supplied set of tuning parameters
models <- svm_classifier(method = "svm",
form = form,
data = data,
kernel = kernel,
error.fun = error.fun,
probability = TRUE,
gamma.set = gamma.set,
cost.set = cost.set,
sampling = "cross",
cross = k.folds,
verbose = TRUE)
# Extract the best model
best_model <- models$best.model
# Extract tuning parameters of best model
out <- list(gamma = best_model$gamma,
cost = best_model$cost)
# Function output
return(out)
}
svm_out <- svm(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
kernel = svm.kernel,
error.fun = svm.error.fun,
gamma.set = svm.gamma.set,
cost.set = svm.cost.set,
k.folds = k.folds,
data = cv_folds,
verbose = verbose)
svm_out
?devtools::install_github
install.packages(c("BH", "broom", "caret", "cli", "data.table", "digest", "dplyr", "fansi", "hms", "ModelMetrics", "pillar", "plyr", "prodlim", "purrr", "R6", "Rcpp", "recipes", "rlang", "scales", "SQUAREM", "stringi", "tidyr", "tidyselect", "vctrs"))
install.packages(c("boot", "foreign", "KernSmooth", "MASS", "Matrix", "mgcv", "nlme", "survival"), lib="/Users/retowuest/Dropbox/MRP and ML/autoMrP/packrat/lib-R/x86_64-apple-darwin15.6.0/3.6.1")

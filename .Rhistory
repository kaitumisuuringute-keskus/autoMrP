if (is.null(pcs)) {
# Determine context-level covariates whose principal components are to be
# computed
if (is.null(pca.L2.x)) {
pca.L2.x <- L2.x
}
# Compute principal components for survey data
pca_out <- stats::prcomp(survey[, pca.L2.x],
retx = TRUE,
center = TRUE,
scale. = TRUE,
tol = NULL)
# Add PCs to survey data
survey <- survey %>%
dplyr::bind_cols(as.data.frame(pca_out$x))
# Add PCs to census data
pc_names <- colnames(pca_out$x)
census <- census %>%
dplyr::left_join(unique(survey %>% dplyr::select(all_of(L2.unit),
all_of(pc_names))),
by = L2.unit)
} else {
pc_names <- pcs
}
# Scale context-level variables in survey and census data
if (isTRUE(L2.x.scale)) {
survey[, L2.x] <- scale(survey[, L2.x], center = TRUE, scale = TRUE)
census[, L2.x] <- scale(census[, L2.x], center = TRUE, scale = TRUE)
}
# Convert survey and census data to tibble
survey <- tibble::as_tibble(x = survey)
census <- tibble::as_tibble(x = census)
if (is.null(folds)) {
# EBMA hold-out fold
ebma.size <- round(nrow(survey) * ebma.size, digits = 0)
if(ebma.size>0){
ebma_folding_out <- ebma_folding(data = survey,
L2.unit = L2.unit,
ebma.size = ebma.size)
ebma_fold <- ebma_folding_out$ebma_fold
cv_data <- ebma_folding_out$cv_data
} else{
ebma_fold <- NULL
cv_data <- survey
}
# K folds for cross-validation
cv_folds <- cv_folding(data = cv_data,
L2.unit = L2.unit,
k.folds = k.folds,
cv.sampling = cv.sampling)
} else {
if (ebma.size > 0){
# EBMA hold-out fold
ebma_fold <- survey %>%
dplyr::filter_at(dplyr::vars(dplyr::one_of(folds)),
dplyr::any_vars(. == k.folds + 1))
}
# K folds for cross-validation
cv_data <- survey %>%
dplyr::filter_at(dplyr::vars(dplyr::one_of(folds)),
dplyr::any_vars(. != k.folds + 1))
cv_folds <- cv_data %>%
dplyr::group_split(.data[[folds]])
}
# Classifier 1: Best Subset
if (isTRUE(best.subset)) {
message("Starting multilevel regression with best subset selection classifier tuning")
# Determine context-level covariates
if (is.null(best.subset.L2.x)) {
best.subset.L2.x <- L2.x
}
# Run classifier
set.seed(seed)
best_subset_out <- run_best_subset(y = y,
L1.x = L1.x,
L2.x = best.subset.L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
best_subset_out <- NULL
}
# Classifier 2: Lasso
if (isTRUE(lasso)) {
message("Starting multilevel regression with L1 regularization tuning")
# Determine context-level covariates
if (is.null(lasso.L2.x)) {
lasso.L2.x <- L2.x
}
# Run classifier
set.seed(seed)
lasso_out <- run_lasso(y = y,
L1.x = L1.x,
L2.x = lasso.L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
lambda = lasso.lambda,
n.iter = lasso.n.iter,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
lasso_out <- NULL
}
# Classifier 3: PCA
if (isTRUE(pca)) {
message("Starting multilevel regression with principal components as context level variables tuning")
set.seed(seed)
pca_out <- run_pca(
y = y,
L1.x = L1.x,
L2.x = pc_names,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
pca_out <- NULL
}
# Classifier 4: GB
if (isTRUE(gb)) {
message("Starting gradient tree boosting tuning")
# Determine context-level covariates
if (is.null(gb.L2.x)) {
gb.L2.x <- L2.x
}
# Evaluate inclusion of L2.unit in GB
if (isTRUE(gb.L2.unit)) {
gb.L2.unit <- L2.unit
} else {
gb.L2.unit <- NULL
}
# Evaluate inclusion of L2.reg in GB
if (isTRUE(gb.L2.reg)) {
gb.L2.reg <- L2.reg
} else {
gb.L2.reg <- NULL
}
# Run classifier
set.seed(seed)
gb_out <- run_gb(y = y,
L1.x = L1.x,
L2.x = gb.L2.x,
L2.unit = gb.L2.unit,
L2.reg = gb.L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
interaction.depth = gb.interaction.depth,
shrinkage = gb.shrinkage,
n.trees.init = gb.n.trees.init,
n.trees.increase = gb.n.trees.increase,
n.trees.max = gb.n.trees.max,
n.iter = gb.n.iter,
n.minobsinnode = gb.n.minobsinnode,
data = cv_folds,
cores = cores,
verbose = verbose)
} else {
gb_out <- NULL
}
# Classifier 5: SVM
if (isTRUE(svm)) {
message("Starting support vector machine tuning")
# Determine context-level covariates
if (is.null(svm.L2.x)) {
svm.L2.x <- L2.x
}
# Evaluate inclusion of L2.unit in GB
if (isTRUE(svm.L2.unit)) {
svm.L2.unit <- L2.unit
} else {
svm.L2.unit <- NULL
}
# Evaluate inclusion of L2.reg in GB
if (isTRUE(svm.L2.reg)) {
svm.L2.reg <- L2.reg
} else {
svm.L2.reg <- NULL
}
# Run classifier
set.seed(seed)
svm_out <- run_svm(y = y,
L1.x = L1.x,
L2.x = svm.L2.x,
L2.unit = svm.L2.unit,
L2.reg = svm.L2.reg,
kernel = svm.kernel,
loss.fun = loss.fun,
loss.unit = loss.unit,
gamma = svm.gamma,
cost = svm.cost,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
svm_out <- NULL
}
message("Starting post-stratification")
set.seed(seed)
ps_out <- post_stratification(
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
best.subset.opt = best_subset_out,
lasso.opt = lasso_out,
lasso.L2.x = lasso.L2.x,
pca.opt = pca_out,
gb.opt = gb_out,
svm.opt = svm_out,
svm.L2.reg = svm.L2.reg,
svm.L2.unit = svm.L2.unit,
svm.L2.x = svm.L2.x,
mrp.include = mrp,
n.minobsinnode = gb.n.minobsinnode,
L2.unit.include = gb.L2.unit,
L2.reg.include = gb.L2.reg,
kernel = svm.kernel,
mrp.L2.x = mrp.L2.x,
data = cv_data,
ebma.fold = ebma_fold,
census = census,
verbose = verbose
)
ps_out
set.seed(seed)
ebma_out <- ebma(
ebma.fold = ebma_fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
post.strat = ps_out,
n.draws = ebma.n.draws,
tol = ebma.tol,
best.subset.opt = best_subset_out,
pca.opt = pca_out,
lasso.opt = lasso_out,
gb.opt = gb_out,
svm.opt = svm_out,
verbose = verbose,
cores = cores
)
ebma_out
class(ebma_out) <- c("autoMrP", "list")
class(ebma_out$ebma) <- c("autoMrP", "ensemble", class(ebma_out$ebma))
class(ebma_out$classifiers) <- c("autoMrP", "classifiers", class(ebma_out$classifiers))
class(ebma_out$weights) <- c("autoMrP", "weights", class(ebma_out$weights))
exists(ebma_out$weights)
exists("ebma_out$weights")
ebma_out$weights <- NULL
class(ebma_out$weights) <- c("autoMrP", "weights", class(ebma_out$weights))
ebma_out
ebma_out$weights <- "EBMA step skipped (only 1 classifier run)"
class(ebma_out$weights) <- c("autoMrP", "weights", class(ebma_out$weights))
return(ebma_out)
ebma_out
summary(ebma_out)
summary.autoMrP(ebma_out)
summary.autoMrP(ebma_out, format = "simple")
summary(ebma_out$weights)
summary(ebma_out$weights)
summary(ebma_out$weights)
summary.autoMrP(ebma_out$weights)
x = ebma_out$weights
x
if (x == "EBMA step skipped (only 1 classifier run)")
x == "EBMA step skipped (only 1 classifier run)"
stop("Weights are not reported if the EBMA step was skipped. Re-run autoMrP with multiple classifiers.")
summary.autoMrP <- function(x, ci.lvl = 0.95, digits = 4, format = "rst",
classifiers = NULL, n = 10, ...){
# weights
if ( all(c("autoMrP", "weights") %in% class(x)) ){
# error message if weights summary called without running multiple classifiers
if (x == "EBMA step skipped (only 1 classifier run)"){
stop("Weights are not reported if the EBMA step was skipped. Re-run autoMrP with multiple classifiers.")
}
# summary statistics
s_data <- x %>%
tidyr::pivot_longer(
cols = dplyr::everything(),
names_to = "method",
values_to = "estimates") %>%
dplyr::group_by(method) %>%
dplyr::summarise(
min = base::min(estimates, na.rm = TRUE),
quart1 = stats::quantile(x = estimates, probs = 0.25),
median = stats::median(estimates),
mean = base::mean(estimates),
quart3 = stats::quantile(x = estimates, probs = 0.75),
max = base::max(estimates),
.groups = "drop") %>%
dplyr::arrange(dplyr::desc(median))
# weights with uncertainty
if ( all(s_data$median != s_data$min) ){
n <- ifelse(n <= nrow(s_data), yes = n, no = nrow(s_data) )
cat( paste("\n", "# EBMA classifier weights:"), sep = "")
# output table
output_table(
x = s_data[1:n, ],
col.names = c(
"Classifier",
"Min.",
"1st Qu.",
"Median",
"Mean",
"3rd Qu.",
"Max"),
format = format,
digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
} else{
s_data <- dplyr::select(.data = s_data, method, median)
n <- ifelse(n <= nrow(s_data), yes = n, no = nrow(s_data) )
cat( paste("\n", "# EBMA classifier weights:"), sep = "")
output_table(
x = s_data[1:n, ],
col.names = c(
"Classifier",
"Weight"),
format = format,
digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
}
}
# ensemble summary
else if ( all(c("autoMrP", "ensemble") %in% class(x)) ) {
# unit identifier
L2.unit <- names(x)[1]
# summary statistics
s_data <- x %>%
dplyr::group_by(.dots = list(L2.unit)) %>%
dplyr::summarise(
min = base::min(ebma, na.rm = TRUE),
lb = stats::quantile(x = ebma, probs = (1 - ci.lvl)*.5 ),
median = stats::quantile(x = ebma, probs = .5 ),
ub = stats::quantile(x = ebma, probs = ci.lvl + (1 - ci.lvl)*.5 ),
max = base::max(ebma, na.rm = TRUE),
.groups = "drop"
)
# with or without uncertainty
if ( all(s_data$median != s_data$lb) ){
cat( paste("\n", "# EBMA estimates:"), sep = "")
# output table
output_table(
x = s_data[1:n, ],
col.names = c(
L2.unit,
"Min.",
"Lower bound",
"Median",
"Upper bound",
"Max"),
format = format,
digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
} else{
s_data <- dplyr::select(.data = s_data, one_of(L2.unit), median)
n <- ifelse(n <= nrow(s_data), yes = n, no = nrow(s_data) )
cat( paste("\n", "# EBMA estimates:"), sep = "")
output_table(
x = s_data[1:n, ],
col.names = c(L2.unit, "Estimates"),
format = format,
digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
}
}
# classifier summary
else if ( all(c("autoMrP", "classifiers") %in% class(x)) ){
# unit identifier
L2.unit <- names(x)[1]
# multiple classifiers
if (base::is.null(classifiers)){
# point estimates for all classifiers
s_data <- x %>%
dplyr::group_by(.dots = list(L2.unit)) %>%
dplyr::summarise_all(.funs = median )
# output table
ests <- paste(names(x)[-1], collapse = ", ")
n <- ifelse(n <= nrow(s_data), yes = n, no = nrow(s_data) )
cat( paste("\n", "# estimates of classifiers: ", ests), sep = "")
output_table(x = s_data[1:n, ],
col.names = names(s_data),
format = format,
digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
} else{
# summary statistics
s_data <- x %>%
dplyr::select(dplyr::one_of(L2.unit,classifiers)) %>%
dplyr::group_by(.dots = list(L2.unit)) %>%
dplyr::summarise_all(.funs = list(
min = ~ base::min(x = ., na.rm = TRUE),
lb = ~ stats::quantile(x = ., probs = (1 - ci.lvl)*.5 ),
median = ~ stats::median(x = ., na.rm = TRUE ),
ub = ~ stats::quantile(x = ., probs = ci.lvl + (1 - ci.lvl)*.5 ),
max = ~ base::max(x = ., na.rm = TRUE)
))
# with or without uncertainty
if( all(s_data$median != s_data$lb) ){
n <- ifelse(n <= nrow(s_data), yes = n, no = nrow(s_data) )
cat( paste("\n", "# estimates of", classifiers, "classifier"), sep = "")
output_table(
x = s_data[1:n, ],
col.names = c(
L2.unit,
"Min.",
"Lower bound",
"Median",
"Upper bound",
"Max"),
format = format,
digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
} else{
s_data <- dplyr::select(.data = s_data, dplyr::one_of(L2.unit), "median")
n <- ifelse(n <= nrow(s_data), yes = n, no = nrow(s_data) )
cat( paste("\n", "# estimates of", classifiers, "classifier"), sep = "")
output_table(
x = s_data[1:n, ],
col.names = c(L2.unit, "Estimates"),
format = format,
digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
}
}
}
# autoMrP list object
else if ( all(c("autoMrP", "list") %in% class(x)) ){
# unit identifier
L2.unit <- names(x$classifiers)[1]
# if EBMA was run
if( all(x$ebma != "EBMA step skipped (only 1 classifier run)") ){
# summary statistics
s_data <- x$ebma %>%
dplyr::group_by(.dots = list(L2.unit)) %>%
dplyr::summarise_all(.funs = list(
min = ~ base::min(x = ., na.rm = TRUE),
lb = ~ stats::quantile(x = ., probs = (1 - ci.lvl)*.5 ),
median = ~ stats::median(x = ., na.rm = TRUE ),
ub = ~ stats::quantile(x = ., probs = ci.lvl + (1 - ci.lvl)*.5 ),
max = ~ base::max(x = ., na.rm = TRUE)
))
# with or without uncertainty
if( all(s_data$median != s_data$lb) ){
n <- ifelse(n <= nrow(s_data), yes = n, no = nrow(s_data) )
cat( paste("\n", "# EBMA estimates:"), sep = "")
output_table(
x = s_data[1:n, ],
col.names = c( L2.unit, "Min.", "Lower bound", "Median", "Upper bound", "Max"),
format = format,
digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
} else{
s_data <- dplyr::select(.data = s_data, dplyr::one_of(L2.unit), median)
n <- ifelse(n <= nrow(s_data), yes = n, no = nrow(s_data) )
cat( paste("\n", "# EBMA estimates:"), sep = "")
output_table(x = s_data[1:n, ], col.names = c(L2.unit, "Median"), format = format, digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
}
} else{
# summary statistics
s_data <- x$classifiers %>%
dplyr::group_by(.dots = list(L2.unit)) %>%
dplyr::summarise_all(.funs = list(
min = ~ base::min(x = ., na.rm = TRUE),
lb = ~ stats::quantile(x = ., probs = (1 - ci.lvl)*.5 ),
median = ~ stats::median(x = ., na.rm = TRUE ),
ub = ~ stats::quantile(x = ., probs = ci.lvl + (1 - ci.lvl)*.5 ),
max = ~ base::max(x = ., na.rm = TRUE)
))
# with or without uncertainty
if ( all(s_data$median != s_data$lb) ){
n <- ifelse(n <= nrow(s_data), yes = n, no = nrow(s_data) )
cat( paste("\n", "# ", names(x$classifiers)[2]," estimates:"), sep = "")
output_table(
x = s_data[1:n, ],
col.names = c(
L2.unit,
"Min.",
"Lower bound",
"Median",
"Upper bound",
"Max"),
format = format,
digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
} else{
# drop uncertainty columns
s_data <- dplyr::select(.data = s_data, dplyr::one_of(L2.unit), median)
n <- ifelse(n <= nrow(s_data), yes = n, no = nrow(s_data) )
cat( paste("\n", "# ", names(x$classifiers)[2]," estimates:"), sep = "")
output_table(x = s_data[1:n, ], col.names = c(L2.unit, "Median"), format = format, digits = digits)
if (n < nrow(s_data)) cat( paste("... with", nrow(s_data)-n, " more rows"), sep = "")
}
}
}
}
summary.autoMrP(ebma_out$weights)
library(autoMrP)

as.numeric(attr(predict(object = model_svm, newdata = test, probability = TRUE),"probabilities")[,"1"])
} else{NA},
mrp = if(!is.null(model_mrp)){
predict(object = model_mrp, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA}
)
# remove NA's
test_preds <- tidyr::drop_na(data = test_preds)
# outcome on the test
# test_y <- dplyr::select(.data = test, one_of(y))
test_y <- dplyr::pull(.data = test, y)
# EBMA
if(verbose){
forecast.data <- EBMAforecast::makeForecastData(
.predCalibration = data.frame(train_preds),
.outcomeCalibration = train_y,
.predTest = data.frame(test_preds),
.outcomeTest = test_y)
forecast.out <- EBMAforecast::calibrateEnsemble(
forecast.data,
model = "normal",
useModelParams = FALSE,
tol = tol[idx.tol])
} else {
forecast.data <- quiet(
EBMAforecast::makeForecastData(
.predCalibration = data.frame(train_preds),
.outcomeCalibration = as.numeric(unlist(train_y)),
.predTest = data.frame(test_preds),
.outcomeTest = as.numeric(unlist(test_y)))
)
forecast.out <- quiet(EBMAforecast::calibrateEnsemble(
forecast.data,
model = "normal",
useModelParams = FALSE,
tol = tol[idx.tol]))
}
# mse
mse_collector[idx.Ndraws, idx.tol] <- mean(( as.numeric(unlist(test_y)) -
as.numeric( attributes(forecast.out)$predTest[,1,1]))^2)
# model weights
weights_box[idx.Ndraws, , idx.tol] <- attributes(forecast.out)$modelWeights
# progress
if (verbose) cat(paste("\n","EBMA: ", round(counter / (length(tol) * n.draws),2)*100, "% done",sep=""))
}
}
idx.tol
idx.Ndraws
# Increase counter
counter <- counter +1
# Determine number per group to sample
n_per_group <- as.integer(nrow(ebma.fold) / length(levels(ebma.fold[[L2.unit]])))
# Test set with n_per_group persons per state (with resampling)
test <- ebma.fold %>%
dplyr::group_by_at( .vars = L2.unit ) %>%
dplyr::sample_n( n_per_group, replace = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate_at(.vars = c( L1.x, L2.unit, L2.reg), .funs = as.factor) %>%
dplyr::select( dplyr::one_of(c(y, L1.x, L2.x, L2.unit, L2.reg, pc.names)))
# predict outcomes in test set
test_preds <- dplyr::tibble(
best_subset = if(!is.null(model_bs)){
predict(object = model_bs, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA},
pca = if(!is.null(model_pca)){
predict(object = model_pca, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA},
lasso = if(!is.null(model_lasso)){
as.numeric(predict(object = model_lasso, newdata = data.frame(test), type = "response"))
} else{NA},
gb = if(!is.null(model_gb)){
gbm::predict.gbm(object = model_gb, newdata = test, n.trees = model_gb$n.trees, type = "response")
} else{NA},
svm = if(!is.null(model_svm)){
as.numeric(attr(predict(object = model_svm, newdata = test, probability = TRUE),"probabilities")[,"1"])
} else{NA},
mrp = if(!is.null(model_mrp)){
predict(object = model_mrp, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA}
)
test
best_subset
model_bs
L2.reg
library(autoMrP)
devtools::load_all()
rm(list=ls())
# arguments
seed <- NULL
y = "YES"
L1.x = c("L1x1", "L1x2", "L1x3")
L2.x = c("")
#L2.x = c("L2.x1", "L2.x2")
mrp.L2.x = NULL
L2.unit = "state"
L2.reg = "region"
L2.x.scale = TRUE
pcs = NULL
folds = NULL
bin.proportion = "proportion"
bin.size = NULL
survey = autoMrP::taxes_survey
census = autoMrP::taxes_census
ebma.size = 1/3
k.folds = 5
cv.sampling = "L2 units"
loss.unit = c("individuals", "L2 units")
loss.fun = c("MSE", "f1", "cross-entropy", "msfe")
# switch for classifiers
best.subset = TRUE
lasso = TRUE
pca = TRUE
gb = TRUE
svm = TRUE
# the standard MRP model
mrp = TRUE
forward.select = FALSE
best.subset.L2.x = NULL
lasso.L2.x = NULL
pca.L2.x = NULL
gb.L2.x = NULL
svm.L2.x = NULL
# Standard MrP model L2 variables
gb.L2.unit = FALSE
gb.L2.reg = TRUE
# tuning params lasso
#lasso.lambda = list(c(0.1, 0.3, 1), c(1, 10, 10000))
lasso.lambda = NULL #1/exp(-seq(from = -1, to = 4.5, length = 100))
lasso.n.iter = 100
# tuning params boosting
gb.interaction.depth = c(1, 2, 3)
gb.shrinkage = c(0.04, 0.01, 0.008, 0.005, 0.001)
gb.n.trees.init = 50
gb.n.trees.increase = 50
gb.n.trees.max = 1000
#gb.n.iter = 70
gb.n.minobsinnode = 5
svm.kernel = c("radial", "polynomial")
svm.loss.fun = NULL
svm.cost = NULL #  .1 #c(1, 2, 5, 10)
svm.gamma = NULL
svm.degree = NULL
ebma.n.draws = 1
ebma.tol = c(0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001)
uncertainty = FALSE
boot.iter <- NULL
seed = 2
verbose = FALSE
cores = 6
svm.L2.reg = TRUE
svm.L2.unit = TRUE
oversampling <- FALSE
ranef.test = TRUE
survey <- autoMrP::taxes_survey
census <- autoMrP::taxes_census
# load all package functions
devtools::load_all()
# Call to function doing the error checks
error_checks(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
L2.x.scale = L2.x.scale,
pcs = pcs,
folds = folds,
bin.proportion = bin.proportion,
bin.size = bin.size,
survey = survey,
census = census,
ebma.size = ebma.size,
k.folds = k.folds,
cv.sampling = cv.sampling,
loss.unit = loss.unit,
loss.fun = loss.fun,
best.subset = best.subset,
lasso = lasso,
pca = pca,
gb = gb,
svm = svm,
mrp = mrp,
forward.select = forward.select,
best.subset.L2.x = best.subset.L2.x,
lasso.L2.x = lasso.L2.x,
gb.L2.x = gb.L2.x,
svm.L2.x = svm.L2.x,
mrp.L2.x = mrp.L2.x,
gb.L2.unit = gb.L2.unit,
gb.L2.reg = gb.L2.reg,
lasso.lambda = lasso.lambda,
lasso.n.iter = lasso.n.iter,
uncertainty = uncertainty,
boot.iter = boot.iter,
seed = seed)
# Check seed argument and set seed
if (is.null(seed)) { seed <- 546213978 }
set.seed(seed)
# Coerce individual-level variables and geographic variables to factors in
# survey and census data
survey <- survey %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), .funs = as.factor)
census <- census %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), .funs = as.factor)
# If not provided in census data, calculate bin size and bin proportion for
# each ideal type in a geographic unit
if (is.null(bin.proportion)) {
if (is.null(bin.size)) {
census <- census %>%
dplyr::group_by(.dots = c(L1.x, L2.unit)) %>%
dplyr::summarise(n = dplyr::n())
} else {
census$n <- census[[bin.size]]
}
census <- census %>%
dplyr::group_by(.dots = L2.unit) %>%
dplyr::mutate(prop = n / sum(n))
} else {
census <- census %>%
dplyr::rename(prop = one_of(bin.proportion))
}
# If not provided in survey and census data, compute the principal components
# of context-level variables
if (is.null(pcs) & L2.x != "") {
# Determine context-level covariates whose principal components are to be
# computed
if (is.null(pca.L2.x)) {
pca.L2.x <- L2.x
}
# Compute principal components for survey data
pca_out <- stats::prcomp(survey[, pca.L2.x],
retx = TRUE,
center = TRUE,
scale. = TRUE,
tol = NULL)
# Add PCs to survey data
survey <- survey %>%
dplyr::bind_cols(as.data.frame(pca_out$x))
# Add PCs to census data
pc_names <- colnames(pca_out$x)
census <- census %>%
dplyr::left_join(unique(survey %>% dplyr::select(all_of(L2.unit),
all_of(pc_names))),
by = L2.unit)
} else {
pc_names <- pcs
}
# Scale context-level variables in survey and census data
if (isTRUE(L2.x.scale) & L2.x != "") {
survey[, L2.x] <- as.numeric(scale(survey[, L2.x], center = TRUE, scale = TRUE))
census[, L2.x] <- as.numeric(scale(census[, L2.x], center = TRUE, scale = TRUE))
}
# Convert survey and census data to tibble
survey <- tibble::as_tibble(x = survey)
census <- tibble::as_tibble(x = census)
# Random over-sampling
if ( isTRUE(oversampling) ){
add_rows <- survey %>%
dplyr::group_by( .dots = L2.unit ) %>%
tidyr::nest() %>%
dplyr::mutate(os = purrr:::map(data, function( x ){
n <- nrow(x)
os <- dplyr::group_by(.data = x, !! rlang::sym(y) )
y_1 <- sum(dplyr::pull(.data = os, var = !! rlang::sym(y)))
y_0 <- n - y_1
if (y_1 > 0 & y_0 > 0){
y_needed <- ifelse(test = y_1 > y_0, yes = 0, no = 1)
n_needed <- ifelse(test = y_needed == 0, yes = y_1 - y_0, no = y_0 - y_1)
os <- dplyr::filter(.data = os, !! rlang::sym(y) == y_needed )
os <- dplyr::slice_sample(.data = os, replace = TRUE, n = n_needed)
}
return(os)
})) %>%
tidyr::unnest(os) %>%
dplyr::ungroup()
survey <- dplyr::bind_rows(survey, add_rows)
}
# Remove NAs from DV ------------------------------------------------------
survey <- tidyr::drop_na(survey, dplyr::all_of(y) )
if (is.null(folds)) {
# EBMA hold-out fold
ebma.size <- round(nrow(survey) * ebma.size, digits = 0)
if(ebma.size>0){
ebma_folding_out <- ebma_folding(data = survey,
L2.unit = L2.unit,
ebma.size = ebma.size)
ebma_fold <- ebma_folding_out$ebma_fold
cv_data <- ebma_folding_out$cv_data
} else{
ebma_fold <- NULL
cv_data <- survey
}
# K folds for cross-validation
cv_folds <- cv_folding(data = cv_data,
L2.unit = L2.unit,
k.folds = k.folds,
cv.sampling = cv.sampling)
} else {
if (ebma.size > 0){
# EBMA hold-out fold
ebma_fold <- survey %>%
dplyr::filter_at(dplyr::vars(dplyr::one_of(folds)),
dplyr::any_vars(. == k.folds + 1))
}
# K folds for cross-validation
cv_data <- survey %>%
dplyr::filter_at(dplyr::vars(dplyr::one_of(folds)),
dplyr::any_vars(. != k.folds + 1))
cv_folds <- cv_data %>%
dplyr::group_split(.data[[folds]])
}
# Classifier 1: Best Subset
if (isTRUE(best.subset)) {
message("Starting multilevel regression with best subset selection classifier tuning")
# Determine context-level covariates
if (is.null(best.subset.L2.x)) {
best.subset.L2.x <- L2.x
}
# Run classifier
set.seed(seed)
best_subset_out <- run_best_subset(y = y,
L1.x = L1.x,
L2.x = best.subset.L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
best_subset_out <- NULL
}
# Classifier 2: Lasso
if (isTRUE(lasso) & L2.x != "") {
message("Starting multilevel regression with L1 regularization tuning")
# Determine context-level covariates
if (is.null(lasso.L2.x)) {
lasso.L2.x <- L2.x
}
# Run classifier
set.seed(seed)
lasso_out <- run_lasso(y = y,
L1.x = L1.x,
L2.x = lasso.L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
lambda = lasso.lambda,
n.iter = lasso.n.iter,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
lasso_out <- NULL
}
# Classifier 3: PCA
if (isTRUE(pca) & L2.x != "") {
message("Starting multilevel regression with principal components as context level variables tuning")
set.seed(seed)
pca_out <- run_pca(
y = y,
L1.x = L1.x,
L2.x = pc_names,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
pca_out <- NULL
}
# Classifier 4: GB
if (isTRUE(gb)) {
message("Starting gradient tree boosting tuning")
# Determine context-level covariates
if (is.null(gb.L2.x)) {
gb.L2.x <- L2.x
}
# GB without L2 variables
if (L2.x == "") gb.L2.x <- NULL
# Evaluate inclusion of L2.unit in GB
if (isTRUE(gb.L2.unit)) {
gb.L2.unit <- L2.unit
} else {
gb.L2.unit <- NULL
}
# Evaluate inclusion of L2.reg in GB
if (isTRUE(gb.L2.reg)) {
gb.L2.reg <- L2.reg
} else {
gb.L2.reg <- NULL
}
# Run classifier
set.seed(seed)
gb_out <- run_gb(y = y,
L1.x = L1.x,
L2.x = gb.L2.x,
L2.eval.unit = L2.unit,
L2.unit = gb.L2.unit,
L2.reg = gb.L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
interaction.depth = gb.interaction.depth,
shrinkage = gb.shrinkage,
n.trees.init = gb.n.trees.init,
n.trees.increase = gb.n.trees.increase,
n.trees.max = gb.n.trees.max,
n.minobsinnode = gb.n.minobsinnode,
data = cv_folds,
cores = cores,
verbose = verbose)
} else {
gb_out <- NULL
}
# Classifier 5: SVM
if ( isTRUE(svm) ) {
message("Starting support vector machine tuning")
# Determine context-level covariates
if (is.null(svm.L2.x)) {
svm.L2.x <- L2.x
}
# SVM without L2 variables
if (L2.x == "") svm.L2.x <- NULL
# Evaluate inclusion of L2.unit in GB
if (isTRUE(svm.L2.unit)) {
svm.L2.unit <- L2.unit
} else {
svm.L2.unit <- NULL
}
# Evaluate inclusion of L2.reg in GB
if (isTRUE(svm.L2.reg)) {
svm.L2.reg <- L2.reg
} else {
svm.L2.reg <- NULL
}
# Run classifier
set.seed(seed)
svm_out <- run_svm(
y = y,
L1.x = L1.x,
L2.x = svm.L2.x,
L2.eval.unit = L2.unit,
L2.unit = svm.L2.unit,
L2.reg = svm.L2.reg,
kernel = svm.kernel,
loss.fun = loss.fun,
loss.unit = loss.unit,
gamma = svm.gamma,
cost = svm.cost,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
svm_out <- NULL
}
message("Starting post-stratification")
set.seed(seed)
ps_out <- post_stratification(
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
best.subset.opt = best_subset_out,
lasso.opt = lasso_out,
lasso.L2.x = lasso.L2.x,
pca.opt = pca_out,
gb.opt = gb_out,
svm.opt = svm_out,
svm.L2.reg = svm.L2.reg,
svm.L2.unit = svm.L2.unit,
svm.L2.x = svm.L2.x,
mrp.include = mrp,
n.minobsinnode = gb.n.minobsinnode,
L2.unit.include = gb.L2.unit,
L2.reg.include = gb.L2.reg,
kernel = svm.kernel,
mrp.L2.x = mrp.L2.x,
data = cv_data,
ebma.fold = ebma_fold,
census = census,
verbose = verbose
)
set.seed(seed)
ebma_out <- ebma(
ebma.fold = ebma_fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
pc.names = pc_names,
post.strat = ps_out,
n.draws = ebma.n.draws,
tol = ebma.tol,
best.subset.opt = best_subset_out,
pca.opt = pca_out,
lasso.opt = lasso_out,
gb.opt = gb_out,
svm.opt = svm_out,
verbose = verbose,
cores = cores
)
ebma_out
library(autoMrP)
remove.packages("seissMrP")
remove.packages("swissMrP")
library(autoMrP)
library(autoMrP)

dplyr::any_vars(. != k.folds + 1))
cv_folds <- cv_data %>%
dplyr::group_split(.data[[folds]])
}
cores
message("Starting multilevel regression with L1 regularization tuning")
lasso.L2.x <- L2.x
start1 <- Sys.time()
lasso_out <- run_lasso(y = y,
L1.x = L1.x,
L2.x = lasso.L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
lambda = lasso.lambda,
n.iter = lasso.n.iter,
data = cv_folds,
verbose = verbose,
cores = cores)
end1 <- Sys.time()
difftime(start1, end1, units = "mins")
L2.x = lasso.L2.x
lasso.L2.x
lambda = lasso.lambda
n.iter = lasso.n.iter
data = cv_folds
verbose = verbose
cores = cores
cores
# Context-level fixed effects
L2_fe <- paste(L2.x, collapse = " + ")
L2_fe_form <- as.formula(paste(y, " ~ ", L2_fe, sep = ""))
L2_fe_form
# Individual-level random effects as named list
L1_re <- setNames(
as.list(rep(c(~ 1), times = length(c(L1.x, L2.unit, L2.reg)))),
c(L1.x, L2.unit, L2.reg))
L1_re
!is.list(lambda)
lambda
# Set lambda value to 0
lambda_value <- 0
lambda_value
# Initialize counter for lambda
lambda_no <- 1
lambda_no
# Print lambda value
if (isTRUE(verbose)) {
cat(paste("Lasso: Running lambda w/ value ", lambda_value,
" (lambda no. ", lambda_no, " -- no improvement evaluation)\n",
sep = ""))
}
cores
run_lasso_mc_kfolds
k_errors <- run_lasso_mc_kfolds(
y = y, L1.x = L1.x, L2.x = L2.x,
L2.unit = L2.unit, L2.reg = L2.reg,
loss.unit = loss.unit, loss.fun = loss.fun,
data = data, cores = cores,
loss.function = loss_function,
verbose = TRUE, L2.fe.form = L2.fe.form,
L1.re = L1.re, lambda.value = lambda_value,
lasso.classifier = lasso_classifier)
L2_fe_form
L2_fe_form
L2.fe.form
k_errors <- run_lasso_mc_kfolds(
y = y, L1.x = L1.x, L2.x = L2.x,
L2.unit = L2.unit, L2.reg = L2.reg,
loss.unit = loss.unit, loss.fun = loss.fun,
data = data, cores = cores,
loss.function = loss_function,
verbose = TRUE, L2.fe.form = L2_fe_form,
L1.re = L1.re, lambda.value = lambda_value,
lasso.classifier = lasso_classifier)
L1_re
L1_re
k_errors <- run_lasso_mc_kfolds(
y = y, L1.x = L1.x, L2.x = L2.x,
L2.unit = L2.unit, L2.reg = L2.reg,
loss.unit = loss.unit, loss.fun = loss.fun,
data = data, cores = cores,
loss.function = loss_function,
verbose = TRUE, L2.fe.form = L2_fe_form,
L1.re = L1_re, lambda.value = lambda_value,
lasso.classifier = lasso_classifier)
# Binding for global variables
`%>%` <- dplyr::`%>%`
k <- NULL
# Register cores
cl <- multicore(cores = cores, type = "open", cl = NULL)
cl
# Loop over each fold
k_errors <- foreach::foreach(k = 1:length(data)) %dopar% {
# Split data in training and validation sets
data_train <- dplyr::bind_rows(data[-k])
data_valid <- dplyr::bind_rows(data[k])
# Convert individual-level, geographic unit, and geographic region
# covariates to factor variables in training and validation sets
data_train <- data_train %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), as.factor)
data_valid <- data_valid %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), as.factor)
# Train model using lambda value on kth training set
model_l <- lasso.classifier(L2.fix = L2.fe.form,
L1.re = L1.re,
data.train = data_train,
lambda = as.numeric(lambda.value),
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data.frame(data_valid))
# Evaluate predictions based on loss function
perform_l <- loss.function(pred = pred_l, data.valid = data_valid,
loss.unit = loss.unit,
loss.fun = loss.fun,
y = y, L2.unit = L2.unit)
}
cores
cores = 4
# Loop over each fold
k_errors <- foreach::foreach(k = 1:length(data)) %dopar% {
# Split data in training and validation sets
data_train <- dplyr::bind_rows(data[-k])
data_valid <- dplyr::bind_rows(data[k])
# Convert individual-level, geographic unit, and geographic region
# covariates to factor variables in training and validation sets
data_train <- data_train %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), as.factor)
data_valid <- data_valid %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), as.factor)
# Train model using lambda value on kth training set
model_l <- lasso.classifier(L2.fix = L2.fe.form,
L1.re = L1.re,
data.train = data_train,
lambda = as.numeric(lambda.value),
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data.frame(data_valid))
# Evaluate predictions based on loss function
perform_l <- loss.function(pred = pred_l, data.valid = data_valid,
loss.unit = loss.unit,
loss.fun = loss.fun,
y = y, L2.unit = L2.unit)
}
# Register cores
cl <- multicore(cores = cores, type = "open", cl = NULL)
# Loop over each fold
k_errors <- foreach::foreach(k = 1:length(data)) %dopar% {
# Split data in training and validation sets
data_train <- dplyr::bind_rows(data[-k])
data_valid <- dplyr::bind_rows(data[k])
# Convert individual-level, geographic unit, and geographic region
# covariates to factor variables in training and validation sets
data_train <- data_train %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), as.factor)
data_valid <- data_valid %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), as.factor)
# Train model using lambda value on kth training set
model_l <- lasso.classifier(L2.fix = L2.fe.form,
L1.re = L1.re,
data.train = data_train,
lambda = as.numeric(lambda.value),
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data.frame(data_valid))
# Evaluate predictions based on loss function
perform_l <- loss.function(pred = pred_l, data.valid = data_valid,
loss.unit = loss.unit,
loss.fun = loss.fun,
y = y, L2.unit = L2.unit)
}
library(foreach)
# Loop over each fold
k_errors <- foreach::foreach(k = 1:length(data)) %dopar% {
# Split data in training and validation sets
data_train <- dplyr::bind_rows(data[-k])
data_valid <- dplyr::bind_rows(data[k])
# Convert individual-level, geographic unit, and geographic region
# covariates to factor variables in training and validation sets
data_train <- data_train %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), as.factor)
data_valid <- data_valid %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), as.factor)
# Train model using lambda value on kth training set
model_l <- lasso.classifier(L2.fix = L2.fe.form,
L1.re = L1.re,
data.train = data_train,
lambda = as.numeric(lambda.value),
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data.frame(data_valid))
# Evaluate predictions based on loss function
perform_l <- loss.function(pred = pred_l, data.valid = data_valid,
loss.unit = loss.unit,
loss.fun = loss.fun,
y = y, L2.unit = L2.unit)
}
library(parallel, lib.loc = "C:/Users/Philipp/Documents/github/autoMrP/packrat/lib-R/x86_64-w64-mingw32/4.0.2")
library(parallel, lib.loc = "C:/Users/Philipp/Documents/github/autoMrP/packrat/lib-R/x86_64-w64-mingw32/4.0.2")
# Loop over each fold
k_errors <- foreach::foreach(k = 1:length(data)) %dopar% {
# Split data in training and validation sets
data_train <- dplyr::bind_rows(data[-k])
data_valid <- dplyr::bind_rows(data[k])
# Convert individual-level, geographic unit, and geographic region
# covariates to factor variables in training and validation sets
data_train <- data_train %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), as.factor)
data_valid <- data_valid %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), as.factor)
# Train model using lambda value on kth training set
model_l <- lasso.classifier(L2.fix = L2.fe.form,
L1.re = L1.re,
data.train = data_train,
lambda = as.numeric(lambda.value),
model.family = binomial(link = "probit"),
verbose = verbose)
# Use trained model to make predictions for kth validation set
pred_l <- stats::predict(model_l, newdata = data.frame(data_valid))
# Evaluate predictions based on loss function
perform_l <- loss.function(pred = pred_l, data.valid = data_valid,
loss.unit = loss.unit,
loss.fun = loss.fun,
y = y, L2.unit = L2.unit)
}
warnings()
cores
best.subset.L2.x <- L2.x
# Run classifier
best_subset_out <- run_best_subset(y = y,
L1.x = L1.x,
L2.x = best.subset.L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
data = cv_folds,
verbose = verbose,
cores = cores)
install.packages("doParallel")
warnings()
library(doParallel)
# Run classifier
best_subset_out <- run_best_subset(y = y,
L1.x = L1.x,
L2.x = best.subset.L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
data = cv_folds,
verbose = verbose,
cores = cores)
install.packages("doParallel")
library(doParallel)
install.packages(c("dplyr", "tibble"))
install.packages("devtools")
rm(list=ls())
# arguments
seed <- NULL
y = "YES"
L1.x = c("L1x1", "L1x2", "L1x3")
L2.x = c("L2.x1", "L2.x2", "L2.x3", "L2.x4", "L2.x5", "L2.x6")
#L2.x = c("L2.x1", "L2.x2", "L2.x3")
mrp.L2.x = c("L2.x1", "L2.x2")
L2.unit = "state"
L2.reg = "region"
L2.x.scale = FALSE
pcs = NULL
folds = NULL
bin.proportion = "proportion"
bin.size = NULL
survey = survey
census = census
ebma.size = 1/3
k.folds = 5
cv.sampling = "L2 units"
loss.unit = "individuals"
loss.fun = "MSE"
# switch for classifiers
best.subset = TRUE
lasso = FALSE
pca = TRUE
gb = FALSE
svm = FALSE
# the standard MRP model
mrp = TRUE
forward.select = FALSE
best.subset.L2.x = NULL
lasso.L2.x = NULL
pca.L2.x = NULL
gb.L2.x = NULL
svm.L2.x = NULL
# Standard MrP model L2 variables
gb.L2.unit = FALSE
gb.L2.reg = FALSE
# tuning params lasso
lasso.lambda = list(c(0.1, 0.3, 1), c(1, 10, 10000))
lasso.n.iter = 70
# tuning params boosting
gb.interaction.depth = c(1, 2, 3)
gb.shrinkage = c(0.04, 0.01, 0.008, 0.005, 0.001)
gb.n.trees.init = 1
gb.n.trees.increase = 50
gb.n.trees.max = 1000
gb.n.iter = 70
gb.n.minobsinnode = 5
svm.kernel = "radial"
svm.loss.fun = NULL
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4)
svm.cost = c(1, 10)
ebma.n.draws = 5
ebma.tol = c(0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001)
uncertainty = FALSE
seed = NULL
verbose = TRUE
cores = 6
svm.L2.reg = TRUE
svm.L2.unit = FALSE
# data
survey <- autoMrP::survey_item
devtools::install_github(retowuest/autoMrP)
devtools::install_github("retowuest/autoMrP")
devtools::load_all()
# arguments
y = "YES"
L1.x = c("L1x1", "L1x2", "L1x3")
L2.x = c("L2.x1", "L2.x2", "L2.x3", "L2.x4", "L2.x5", "L2.x6")
#L2.x = c("L2.x1", "L2.x2", "L2.x3")
mrp.L2.x = c("L2.x1", "L2.x2")
L2.unit = "state"
L2.reg = "region"
L2.x.scale = FALSE
pcs = NULL
folds = NULL
bin.proportion = "proportion"
bin.size = NULL
survey = survey
census = census
ebma.size = 1/3
k.folds = 5
cv.sampling = "L2 units"
loss.unit = "individuals"
loss.fun = "MSE"
# switch for classifiers
best.subset = TRUE
lasso = TRUE
pca = TRUE
gb = TRUE
svm = TRUE
# the standard MRP model
mrp = TRUE
forward.select = FALSE
best.subset.L2.x = NULL
lasso.L2.x = NULL
pca.L2.x = NULL
gb.L2.x = NULL
svm.L2.x = NULL
# Standard MrP model L2 variables
gb.L2.unit = FALSE
gb.L2.reg = TRUE
# tuning params lasso
lasso.lambda = list(c(0.1, 0.3, 1), c(1, 10, 10000))
lasso.n.iter = 70
# tuning params boosting
gb.interaction.depth = c(1, 2, 3)
gb.shrinkage = c(0.04, 0.01, 0.008, 0.005, 0.001)
gb.n.trees.init = 1
gb.n.trees.increase = 50
gb.n.trees.max = 1000
gb.n.iter = 70
gb.n.minobsinnode = 5
svm.kernel = "radial"
svm.loss.fun = NULL
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4)
svm.cost = c(1, 10)
ebma.n.draws = 5
ebma.tol = c(0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001)
uncertainty = FALSE
seed = NULL
verbose = TRUE
cores = 6
svm.L2.reg = TRUE
svm.L2.unit = FALSE
# arguments
y = "YES"
L1.x = c("L1x1", "L1x2", "L1x3")
L2.x = c("L2.x1", "L2.x2", "L2.x3", "L2.x4", "L2.x5", "L2.x6")
#L2.x = c("L2.x1", "L2.x2", "L2.x3")
#mrp.L2.x = c("L2.x1", "L2.x2")
L2.unit = "state"
L2.reg = "region"
L2.x.scale = FALSE
pcs = NULL
folds = NULL
bin.proportion = "proportion"
bin.size = NULL
survey = survey
census = census
ebma.size = 1/3
k.folds = 5
cv.sampling = "L2 units"
loss.unit = "individuals"
loss.fun = "MSE"
# switch for classifiers
best.subset = FALSE
lasso = TRUE
pca = FALSE
gb = FALSE
svm = FALSE
# the standard MRP model
mrp = FALSE
forward.select = FALSE
best.subset.L2.x = NULL
lasso.L2.x = NULL
pca.L2.x = NULL
gb.L2.x = NULL
svm.L2.x = NULL
# Standard MrP model L2 variables
gb.L2.unit = FALSE
gb.L2.reg = TRUE
# tuning params lasso
lasso.lambda = list(c(0.1, 0.3, 1), c(1, 10, 10000))
lasso.lambda <- 1 / exp(- seq(from = -1, to = 4.5, length = 100))
lasso.n.iter = 70
# tuning params boosting
gb.interaction.depth = c(1, 2, 3)
gb.shrinkage = c(0.04, 0.01, 0.008, 0.005, 0.001)
gb.n.trees.init = 1
gb.n.trees.increase = 50
gb.n.trees.max = 1000
gb.n.iter = 70
gb.n.minobsinnode = 5
svm.kernel = "radial"
svm.loss.fun = NULL
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4)
svm.cost = c(1, 10)
ebma.n.draws = 5
ebma.tol = c(0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001)
uncertainty = FALSE
seed = NULL
verbose = TRUE
cores = 6
svm.L2.reg = TRUE
svm.L2.unit = FALSE
start <- Sys.time()
out <- autoMrP::auto_MrP(
cores = 6,
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3", "L2.x4", "L2.x5", "L2.x6"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = FALSE,
lasso = TRUE,
pca = FALSE,
gb = FALSE,
svm = FALSE,
mrp = FALSE,
#ebma.tol = c(0.001, 0.0005),
#svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
#svm.cost = c(0.1, 0.5, 1, 5, 10, 20, 50, 100),
#svm.L2.unit = FALSE,
#svm.L2.reg = FALSE,
#ebma.n.draws = 2,
#gb.L2.reg = TRUE,
lasso.n.iter = 70
)
end <- Sys.time()
start2 <- Sys.time()
out <- autoMrP::auto_MrP(
cores = 6,
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2", "L2.x3", "L2.x4", "L2.x5", "L2.x6"),
L2.unit = "state",
L2.reg = "region",
L2.x.scale = FALSE,
survey = autoMrP::survey_item,
census = autoMrP::census,
bin.proportion = "proportion",
best.subset = FALSE,
lasso = TRUE,
pca = FALSE,
gb = FALSE,
svm = FALSE,
mrp = FALSE,
#ebma.tol = c(0.001, 0.0005),
#svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
#svm.cost = c(0.1, 0.5, 1, 5, 10, 20, 50, 100),
#svm.L2.unit = FALSE,
#svm.L2.reg = FALSE,
#ebma.n.draws = 2,
#gb.L2.reg = TRUE,
lasso.n.iter = 70,
lasso.lambda = 1 / exp(- seq(from = -1, to = 4.5, length = 100))
)
end2 <- Sys.time()
difftime(time1 = start, time2 = end, "mins")
difftime(time1 = end, time2 = start, "mins")
difftime(time1 = end2, time2 = start2, "mins")
plot(1 / exp(- seq(from = -1, to = 4.5, length = 100)))
1 / exp(- seq(from = -1, to = 4.5, length = 100))

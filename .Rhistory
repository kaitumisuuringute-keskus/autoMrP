svm.opt = svm_out,
mrp.include = mrp,
n.minobsinnode = gb.n.minobsinnode,
L2.unit.include = gb.L2.unit,
L2.reg.include = gb.L2.reg,
kernel = svm.kernel,
mrp.L2.x = mrp.L2.x,
data = cv_data,
ebma.fold = ebma_fold,
census = census,
verbose = verbose
)
ps_out
library(autoMrP)
?census
library(autoMrP)
library(autoMrP)
warnings()
library(autoMrP)
packageStartupMessage("Predictions may take some time. On default settings, the example data runs in 30 minutes with 6 context-level variables.")
packageStartupMessage("Predictions may take some time. On default settings, the example data runs in ~30 minutes with 6 context-level variables.")
library(autoMrP)
?sum
devtools::load_all()
dat <- survey_item
names(survey_item)
models <- model_list_pca(y = "YES", L1.x = c("L1x1", "L1x2", "L1x3"),
L2.unit = "state", L2.reg = "region", L2.x = c("L2.x1", "L2.x2"))
models
best_subset_classifier()
best_subset_classifier
models[[1]]
m <- best_subset_classifier(
model = YES ~ (1 | L1x1) + (1 | L1x2) + (1 | L1x3) + (1 | region/state),
data.train = survey_item,
model.family = binomial(link = "probit"),
model.optimizer = "bobyqa",
n.iter = 1000000,
verbose = TRUE)
m
m <- best_subset_classifier(
model = YES ~ (1 | L1x1) + (1 | L1x2) + (1 | L1x3) + (1 | region/state) + L2.x1 + L2.x2,
data.train = survey_item,
model.family = binomial(link = "probit"),
model.optimizer = "bobyqa",
n.iter = 1000000,
verbose = TRUE)
m
m <- best_subset_classifier(
model = YES ~ (1 | L1x1) + (1 | L1x2) + (1 | L1x3) + (1 | region/state) + L2.x1,
data.train = survey_item,
model.family = binomial(link = "probit"),
model.optimizer = "bobyqa",
n.iter = 1000000,
verbose = TRUE)
m <- best_subset_classifier(
model = YES ~ (1 | L1x1) + (1 | L1x2) + (1 | L1x3) + (1 | region/state) + L2.x1 + L2.x2,
data.train = survey_item,
model.family = binomial(link = "probit"),
model.optimizer = "bobyqa",
n.iter = 1000000,
verbose = TRUE)
m
m <- gb_classifier(
form = YES ~ L1x1 + L1x2 + L1x3 + state + region + L2.x1 + L2.x2,
distribution = "bernoulli",
data.train = survey_item,
n.trees = 100,
interaction.depth = 2,
n.minobsinnode = 5,
shrinkage = 0.01,
verbose = verbose)
m <- gb_classifier(
form = YES ~ L1x1 + L1x2 + L1x3 + state + region + L2.x1 + L2.x2,
distribution = "bernoulli",
data.train = survey_item,
n.trees = 100,
interaction.depth = 2,
n.minobsinnode = 5,
shrinkage = 0.01,
verbose = TRUE)
m
?auto_MrP
# Context-level fixed effects
L2_fe <- paste(L2.x, collapse = " + ")
L2.x <- c("L2.x1", "L2.x2")
# Context-level fixed effects
L2_fe <- paste(L2.x, collapse = " + ")
L2_fe_form <- as.formula(paste(y, " ~ ", L2_fe, sep = ""))
y = "YES"
L2_fe_form <- as.formula(paste(y, " ~ ", L2_fe, sep = ""))
L2_fe_form
# Individual-level random effects as named list
L1_re <- setNames(as.list(rep(c(~ 1),
times = length(c(L1.x, L2.unit, L2.reg)))),
c(L1.x, L2.unit, L2.reg))
L1.x <- c("L1.x1", "L1.x2)
)
L2.unit = "state"
L2.unit = "state"
L1.x <- c("L1.x1", "L1.x2")
L2.reg = "region"
# Individual-level random effects as named list
L1_re <- setNames(as.list(rep(c(~ 1),
times = length(c(L1.x, L2.unit, L2.reg)))),
c(L1.x, L2.unit, L2.reg))
L1_re
m <- lasso_classifier(
L2.fix = YES ~ L2.x1 + L2.x2,
L1.re = list(L1.x1 = ~1, L1.x2 = ~1, state = ~1, region = ~1),
data.train = survey_item,
lambda = 5,
model.family = binomial(link = "probit"),
verbose = TRUE)
names(survey_item)
m <- lasso_classifier(
L2.fix = YES ~ L2.x1 + L2.x2,
L1.re = list(L1x1 = ~1, L1x2 = ~1, state = ~1, region = ~1),
data.train = survey_item,
lambda = 5,
model.family = binomial(link = "probit"),
verbose = TRUE)
m
# arguments
y = "YES"
L1.x = c("L1x1", "L1x2", "L1x3")
#L2.x = c("L2.x1", "L2.x2", "L2.x3", "L2.x4", "L2.x5", "L2.x6")
L2.x = c("L2.x1", "L2.x2")
mrp.L2.x = c("L2.x1", "L2.x2")
L2.unit = "state"
L2.reg = "region"
L2.x.scale = TRUE
pcs = NULL
folds = NULL
bin.proportion = "proportion"
bin.size = NULL
survey = survey
census = census
ebma.size = 1/3
k.folds = 5
cv.sampling = "L2 units"
loss.unit = "individuals"
loss.fun = "MSE"
# switch for classifiers
best.subset = FALSE
lasso = FALSE
pca = TRUE
gb = TRUE
svm = FALSE
# the standard MRP model
mrp = TRUE
forward.select = FALSE
best.subset.L2.x = NULL
lasso.L2.x = NULL
pca.L2.x = NULL
gb.L2.x = NULL
svm.L2.x = NULL
# Standard MrP model L2 variables
mrp.L2.x = NULL
gb.L2.unit = FALSE
gb.L2.reg = FALSE
# tuning params lasso
lasso.lambda = list(c(0.1, 0.3, 1), c(1, 10, 10000))
lasso.n.iter = 70
# tuning params boosting
gb.interaction.depth = c(1, 2, 3)
gb.shrinkage = c(0.04, 0.01, 0.008, 0.005, 0.001)
gb.n.trees.init = 50
gb.n.trees.increase = 50
gb.n.trees.max = 1000
gb.n.iter = 70
gb.n.minobsinnode = 5
svm.kernel = "radial"
svm.loss.fun = NULL
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4)
svm.cost = c(1, 10)
ebma.n.draws = 100
ebma.tol = c(0.01, 0.005, 0.001, 5e-04, 1e-04, 5e-05, 1e-05)
uncertainty = FALSE
seed = NULL
verbose = TRUE
# Check seed argument and set seed
if (is.null(seed)) {
set.seed(546213978)
} else {
if (isTRUE(dplyr::near(seed, as.integer(seed)))) {
set.seed(seed)
} else {
stop("Seed must be either NULL or an integer-valued scalar.")
}
}
# Call to function doing the error checks
error_checks(y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
L2.x.scale = L2.x.scale,
pcs = pcs,
folds = folds,
bin.proportion = bin.proportion,
bin.size = bin.size,
survey = survey,
census = census,
ebma.size = ebma.size,
k.folds = k.folds,
cv.sampling = cv.sampling,
loss.unit = loss.unit,
loss.fun = loss.fun,
best.subset = best.subset,
lasso = lasso,
pca = pca,
gb = gb,
svm = svm,
mrp = mrp,
forward.select = forward.select,
best.subset.L2.x = best.subset.L2.x,
lasso.L2.x = lasso.L2.x,
gb.L2.x = gb.L2.x,
svm.L2.x = svm.L2.x,
mrp.L2.x = mrp.L2.x,
gb.L2.unit = gb.L2.unit,
gb.L2.reg = gb.L2.reg,
lasso.lambda = lasso.lambda,
lasso.n.iter = lasso.n.iter)
# Coerce individual-level variables and geographic variables to factors in
# survey and census data
survey <- survey %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), .funs = as.factor)
survey = survey_item
# Coerce individual-level variables and geographic variables to factors in
# survey and census data
survey <- survey %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), .funs = as.factor)
census <- census %>%
dplyr::mutate_at(.vars = c(L1.x, L2.unit, L2.reg), .funs = as.factor)
# If not provided in census data, calculate bin size and bin proportion for
# each ideal type in a geographic unit
if (is.null(bin.proportion)) {
if (is.null(bin.size)) {
census <- census %>%
dplyr::group_by(.dots = c(L1.x, L2.unit)) %>%
dplyr::summarise(n = dplyr::n())
} else {
census$n <- census[[bin.size]]
}
census <- census %>%
dplyr::group_by(.dots = L2.unit) %>%
dplyr::mutate(prop = n / sum(n))
} else {
census <- census %>%
dplyr::rename(prop = one_of(bin.proportion))
}
# If not provided in survey and census data, compute the principal components
# of context-level variables
if (is.null(pcs)) {
# Determine context-level covariates whose principal components are to be
# computed
if (is.null(pca.L2.x)) {
pca.L2.x <- L2.x
}
# Compute principal components for survey data
pca_out <- stats::prcomp(survey[, pca.L2.x],
retx = TRUE,
center = TRUE,
scale. = TRUE,
tol = NULL)
# Add PCs to survey data
survey <- survey %>%
dplyr::bind_cols(as.data.frame(pca_out$x))
# Add PCs to census data
pc_names <- colnames(pca_out$x)
census <- census %>%
dplyr::left_join(unique(survey %>% dplyr::select(all_of(L2.unit),
all_of(pc_names))),
by = L2.unit)
} else {
pc_names <- pcs
}
# Scale context-level variables in survey and census data
if (isTRUE(L2.x.scale)) {
survey[, L2.x] <- scale(survey[, L2.x], center = TRUE, scale = TRUE)
census[, L2.x] <- scale(census[, L2.x], center = TRUE, scale = TRUE)
}
# Convert survey and census data to tibble
survey <- tibble::as_tibble(x = survey)
census <- tibble::as_tibble(x = census)
if (is.null(folds)) {
# EBMA hold-out fold
ebma.size <- round(nrow(survey) * ebma.size, digits = 0)
ebma_folding_out <- ebma_folding(data = survey,
L2.unit = L2.unit,
ebma.size = ebma.size)
ebma_fold <- ebma_folding_out$ebma_fold
cv_data <- ebma_folding_out$cv_data
# K folds for cross-validation
cv_folds <- cv_folding(data = cv_data,
L2.unit = L2.unit,
k.folds = k.folds,
cv.sampling = cv.sampling)
} else {
# If svm is TRUE, print warning that SVM classifier does not rely on
# user-specified folds
if (isTRUE(svm)) {
warning(paste("Currently the SVM classifier does not use the user-supplied",
" folds. This will be added in the next version of the",
" package.", sep = ""))
}
# EBMA hold-out fold
ebma_fold <- survey %>%
dplyr::filter_at(dplyr::vars(dplyr::one_of(folds)),
dplyr::any_vars(. == k.folds + 1))
# K folds for cross-validation
cv_data <- survey %>%
dplyr::filter_at(dplyr::vars(dplyr::one_of(folds)),
dplyr::any_vars(. != k.folds + 1))
cv_folds <- cv_data %>%
dplyr::group_split(.data[[folds]])
}
L2.x
# Run classifier
best_subset_out <- run_best_subset(y = y,
L1.x = L1.x,
L2.x = best.subset.L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
data = cv_folds,
verbose = verbose)
loss.unit
loss.fun
cv_folds
out <- run_best_subset(
y = "YES",
L1.x = c("L1x1", "L1x2", "L1x3"),
L2.x = c("L2.x1", "L2.x2"),
L2.unit = "state",
L2.reg = "region",
loss.unit = "individuals",
loss.fun = "MSE",
data = cv_folds,
verbose = TRUE)
survey_item
cv_folds
survey_item
cv_folds
cv_folds <- list(
1 = survey_item[1:200, ],
2 = survey_item[201:400, ],
3 = survey_item[401:1500, ]
)
cv_folds <- list(
`1` = survey_item[1:200, ],
`2` = survey_item[201:400, ],
`3` = survey_item[401:1500, ]
)
cv_folds
out <- run_best_subset(
y = "YES",
L1.x = c("L1x1", "L1x2", "L1x3"),
L2.x = c("L2.x1", "L2.x2"),
L2.unit = "state",
L2.reg = "region",
loss.unit = "individuals",
loss.fun = "MSE",
data = cv_folds,
verbose = TRUE)
out
# Run classifier
gb_out <- run_gb(y = y,
L1.x = L1.x,
L2.x = gb.L2.x,
L2.unit = gb.L2.unit,
L2.reg = gb.L2.reg,
loss.unit = loss.unit,
loss.fun = loss.fun,
interaction.depth = gb.interaction.depth,
shrinkage = gb.shrinkage,
n.trees.init = gb.n.trees.init,
n.trees.increase = gb.n.trees.increase,
n.trees.max = gb.n.trees.max,
n.iter = gb.n.iter,
n.minobsinnode = gb.n.minobsinnode,
data = cv_folds,
verbose = verbose)
loss.unit
loss.fun
gb.interaction.depth
gb.shrinkage
gb.n.trees.init
gb.n.trees.increase
gb.n.trees.max
gb.n.iter
gb.n.minobsinnode
# create list of cross-validation folds
cv_folds <- list(
`1` = survey_item[1:200, ],
`2` = survey_item[201:400, ],
`3` = survey_item[401:1500, ])
# run gradient boosting classifier
out <- run_gb(
y = "YES",
L1.x = c("L1x1", "L1x2"),
L2.x = c("L2.x1", "L2.x2"),
L2.unit = NULL,
L2.reg = "region",
loss.unit = "individuals",
loss.fun = "MSE",
interaction.depth = c(1, 2, 3),
shrinkage = c(0.04, 0.01),
n.trees.init = 50,
n.trees.increase = 50,
n.trees.max = 1000,
n.iter = 70,
n.minobsinnode = 5,
data = cv_folds,
verbose = TRUE)
out
library(autoMrP)
library(autoMrP)
library(autoMrP)
library(autoMrP)
library(autoMrP)
library(autoMrP)
rm(list=ls())
library(dplyr)
# load data
load("C:/Users/Philipp/Documents/github/autoMrP/data/survey.RData")
#survey <- dplyr::select(survey, -contains("PC"), -"fold", -"id")
survey
load("C:/Users/Philipp/Documents/github/autoMrP/data/census.RData")
setwd("C:/Users/Philipp/Documents/github/autoMrP/")
source("./R/best_subset_classifier.R")
source("./R/ebma.R")
source("./R/gb_classifier.R")
source("./R/lasso_classifier.R")
source("./R/post_stratification.R")
source("./R/run_best_subset.R")
source("./R/run_gb.R")
source("./R/run_lasso.R")
source("./R/run_pca.R")
source("./R/run_svm.R")
source("./R/svm_classifier.R")
source("./R/utils.R")
# arguments
y = "YES"
L1.x = c("L1x1", "L1x2", "L1x3")
#L2.x = c("L2.x1", "L2.x2", "L2.x3", "L2.x4", "L2.x5", "L2.x6")
L2.x = c("L2.x1", "L2.x2")
mrp.L2.x = c("L2.x1", "L2.x2")
L2.unit = "state"
L2.reg = "region"
L2.x.scale = TRUE
pcs = NULL
folds = NULL
bin.proportion = "proportion"
bin.size = NULL
survey = survey
census = census
ebma.size = 1/3
k.folds = 5
cv.sampling = "L2 units"
loss.unit = "individuals"
loss.fun = "MSE"
# switch for classifiers
best.subset = FALSE
lasso = FALSE
pca = TRUE
gb = TRUE
svm = FALSE
# the standard MRP model
mrp = TRUE
forward.select = FALSE
best.subset.L2.x = NULL
lasso.L2.x = NULL
pca.L2.x = NULL
gb.L2.x = NULL
svm.L2.x = NULL
# Standard MrP model L2 variables
mrp.L2.x = NULL
gb.L2.unit = FALSE
gb.L2.reg = FALSE
# tuning params lasso
lasso.lambda = list(c(0.1, 0.3, 1), c(1, 10, 10000))
lasso.n.iter = 70
# tuning params boosting
gb.interaction.depth = c(1, 2, 3)
gb.shrinkage = c(0.04, 0.01, 0.008, 0.005, 0.001)
gb.n.trees.init = 50
gb.n.trees.increase = 50
gb.n.trees.max = 1000
gb.n.iter = 70
gb.n.minobsinnode = 5
svm.kernel = "radial"
svm.loss.fun = NULL
svm.gamma = c(0.3, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9, 1, 2, 3, 4)
svm.cost = c(1, 10)
ebma.n.draws = 100
ebma.tol = c(0.01, 0.005, 0.001, 5e-04, 1e-04, 5e-05, 1e-05)
uncertainty = FALSE
seed = NULL
verbose = TRUE
devtools::load_all()
rm(list=ls())
library(dplyr)
# load data
load("C:/Users/Philipp/Documents/github/autoMrP/data/survey.RData")
#survey <- dplyr::select(survey, -contains("PC"), -"fold", -"id")
survey
load("C:/Users/Philipp/Documents/github/autoMrP/data/census.RData")
#census <- dplyr::select(census, -contains("PC"))

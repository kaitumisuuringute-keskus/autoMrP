svm_out <- run_svm(
y = y,
L1.x = L1.x,
L2.x = svm.L2.x,
L2.eval.unit = L2.unit,
L2.unit = svm.L2.unit,
L2.reg = svm.L2.reg,
kernel = svm.kernel,
loss.fun = loss.fun,
loss.unit = loss.unit,
gamma = svm.gamma,
cost = svm.cost,
data = cv_folds,
verbose = verbose,
cores = cores)
} else {
svm_out <- NULL
}
# Post-stratification -----------------------------------------------------
message("Starting post-stratification")
set.seed(seed)
ps_out <- post_stratification(
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
best.subset.opt = best_subset_out,
lasso.opt = lasso_out,
lasso.L2.x = lasso.L2.x,
pca.opt = pca_out,
gb.opt = gb_out,
svm.opt = svm_out,
svm.L2.reg = svm.L2.reg,
svm.L2.unit = svm.L2.unit,
svm.L2.x = svm.L2.x,
mrp.include = mrp,
n.minobsinnode = gb.n.minobsinnode,
L2.unit.include = gb.L2.unit,
L2.reg.include = gb.L2.reg,
kernel = svm.kernel,
mrp.L2.x = mrp.L2.x,
data = cv_data,
ebma.fold = ebma_fold,
census = census,
verbose = verbose
)
# EBMA --------------------------------------------------------------------
set.seed(seed)
ebma_out <- ebma(
ebma.fold = ebma_fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
pc.names = pc_names,
post.strat = ps_out,
n.draws = ebma.n.draws,
tol = ebma.tol,
best.subset.opt = best_subset_out,
pca.opt = pca_out,
lasso.opt = lasso_out,
gb.opt = gb_out,
svm.opt = svm_out,
verbose = verbose,
cores = cores
)
class(ebma_out) <- c("autoMrP", "list")
class(ebma_out$ebma) <- c("autoMrP", "ensemble", class(ebma_out$ebma))
class(ebma_out$classifiers) <- c("autoMrP", "classifiers", class(ebma_out$classifiers))
if ("weights" %in% names(ebma_out)){
class(ebma_out$weights) <- c("autoMrP", "weights", class(ebma_out$weights))
} else{
ebma_out$weights <- "EBMA step skipped (only 1 classifier run)"
class(ebma_out$weights) <- c("autoMrP", "weights", class(ebma_out$weights))
}
x = ebma_out
ebma_out
algorithm = "ebma"
ci.lvl = 0.95
names(x$classifiers)[1]
ebma.fold = ebma_fold
y = y
L1.x = L1.x
L2.x = L2.x
L2.unit = L2.unit
L2.reg = L2.reg
pc.names = pc_names
post.strat = ps_out
n.draws = ebma.n.draws
tol = ebma.tol
best.subset.opt = best_subset_out
pca.opt = pca_out
lasso.opt = lasso_out
gb.opt = gb_out
svm.opt = svm_out
verbose = verbose
cores = core
# Binding for global variables
`%>%` <- dplyr::`%>%`
# Register cores
cl <- multicore(cores = cores, type = "open", cl = NULL)
ebma_tune <- foreach::foreach(idx.Ndraws = 1:n.draws, .packages = c("glmmLasso", "e1071", "gbm")) %dorng% {
# Determine number per group to sample
n_per_group <- as.integer(nrow(ebma.fold) / length(levels(ebma.fold[[L2.unit]])))
# Test set with n_per_group persons per state (with resampling)
test <- ebma.fold %>%
dplyr::group_by_at( .vars = L2.unit ) %>%
dplyr::sample_n( n_per_group, replace = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate_at(.vars = c( L1.x, L2.unit, L2.reg), .funs = as.factor) %>%
dplyr::select( dplyr::one_of(c(y, L1.x, L2.x, L2.unit, L2.reg, pc.names))) %>%
tidyr::drop_na()
# predict outcomes in test set
test_preds <- dplyr::tibble(
best_subset = if(!is.null(model.bs)){
predict(object = model.bs, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA},
pca = if(!is.null(model.pca)){
predict(object = model.pca, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA},
lasso = if(!is.null(model.lasso)){
predict_glmmLasso(census = test, m = model.lasso, L1.x = L1.x, lasso.L2.x = L2.x, L2.unit = L2.unit, L2.reg = L2.reg)
} else{NA},
gb = if(!is.null(model.gb)){
gbm::predict.gbm(object = model.gb, newdata = test, n.trees = model.gb$n.trees, type = "response")
} else{NA},
svm = if(!is.null(model.svm)){
as.numeric(attr(predict(object = model.svm, newdata = test, probability = TRUE),"probabilities")[,"1"])
} else{NA},
mrp = if(!is.null(model.mrp)){
predict(object = model.mrp, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA}
)
# Remove NA's
test_preds <- test_preds[,apply(X = test_preds, MARGIN = 2, FUN = function(x){
all(!is.na(x))})]
# Outcome on the test
test_y <- dplyr::select(.data = test, one_of(y))
# Container of MSEs per tolerance value and draw combination
mse_collector <- NA
# Container of model weights over tolerances in current draw
weights_box <- matrix(NA, nrow = length(tol), ncol = ncol(train.preds),
dimnames = list(c(paste0("Tol_" ,tol) ),
c(names(train.preds))))
# Loop over tolerance values
for (idx.tol in 1:length(tol)){
# EBMA
forecast.data <- suppressWarnings(
EBMAforecast::makeForecastData(
.predCalibration = data.frame(train.preds),
.outcomeCalibration = as.numeric(unlist(train.y)),
.predTest = data.frame(test_preds),
.outcomeTest = as.numeric(unlist(test_y)))
)
forecast.out <- EBMAforecast::calibrateEnsemble(
forecast.data,
model = "normal",
useModelParams = FALSE,
tol = tol[idx.tol])
# mse
mse_collector[idx.tol] <- mean(( as.numeric(unlist(test_y)) -
as.numeric( attributes(forecast.out)$predTest[,1,1]))^2)
# model weights
weights_box[idx.tol, ] <- attributes(forecast.out)$modelWeights
}
return(list(MSEs = mse_collector, weights = weights_box))
}
message("Starting bayesian ensemble model averaging tuning")
# EBMA wihtout L2.x variables
if (all(L2.x == "")) L2.x <- NULL
# Models
model_bs <- post.strat$models$best_subset
model_pca <- post.strat$models$pca
model_lasso <- post.strat$models$lasso
model_gb <- post.strat$models$gb
model_svm <- post.strat$models$svm
model_mrp <- post.strat$models$mrp
# Training predictions
train_preds <- post.strat$predictions$Level1 %>%
dplyr::select(-one_of(y))
# Training set outcomes
train_y <- dplyr::pull(.data = post.strat$predictions$Level1, var = y)
# Parallel tuning, if cores > 1
if (cores > 1){
# Distribute clusters over tolerance values or n.draws
if(length(tol) <= n.draws*3){
final_model_weights <- ebma_mc_draws(
train.preds = train_preds,
train.y = train_y,
ebma.fold = ebma.fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
pc.names = pc.names,
model.bs = model_bs,
model.pca = model_pca,
model.lasso = model_lasso,
model.gb = model_gb,
model.svm = model_svm,
model.mrp = model_mrp,
tol = tol,
n.draws = n.draws,
cores = cores)
} else {
final_model_weights <- ebma_mc_tol(
train.preds = train_preds,
train.y = train_y,
ebma.fold = ebma.fold,
y = y,
L1.x = L1.x,
L2.x = L2.x,
L2.unit = L2.unit,
L2.reg = L2.reg,
pc.names = pc.names,
model.bs = model_bs,
model.pca = model_pca,
model.lasso = model_lasso,
model.gb = model_gb,
model.svm = model_svm,
model.mrp = model_mrp,
tol = tol,
n.draws = n.draws,
cores = cores)
}
} else{
# Counter for verbose screen output
counter <- 0
# Container to store the MSE on the test folds
# Bootstrap draws in rows and tolerance values in columns
mse_collector <- matrix(
NA, nrow = n.draws,
ncol = length(tol),
dimnames = list( c(paste0("Ndraw_", seq(1:n.draws))),
c(paste0("Tol: ", tol) )))
# container for model weights for each draw and tolerance value
# Dimension 1 (rows): Bootstrap draws
# Dimension 2 (columns): Classifiers
# Dimension 3 (layers): Tolerance values
weights_box <- array(
NA,
dim = c(n.draws, ncol(train_preds), length(tol)),
dimnames = list( c(paste0("Ndraw_", seq(1:n.draws))),
c(colnames(train_preds)),
c(paste0("Tol: ", tol))))
# loop over tolerance values
for (idx.tol in 1:length(tol)){
# loop over Ndraws wit equal obs/state
for (idx.Ndraws in 1:n.draws){
# Increase counter
counter <- counter +1
# Determine number per group to sample
n_per_group <- as.integer(nrow(ebma.fold) / length(levels(ebma.fold[[L2.unit]])))
# Test set with n_per_group persons per state (with resampling)
test <- ebma.fold %>%
dplyr::group_by_at( .vars = L2.unit ) %>%
dplyr::sample_n( n_per_group, replace = TRUE) %>%
dplyr::ungroup() %>%
dplyr::mutate_at(.vars = c( L1.x, L2.unit, L2.reg), .funs = as.factor) %>%
dplyr::select( dplyr::one_of(c(y, L1.x, L2.x, L2.unit, L2.reg, pc.names))) %>%
tidyr::drop_na()
# predict outcomes in test set
test_preds <- dplyr::tibble(
best_subset = if(!is.null(model_bs)){
predict(object = model_bs, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA},
pca = if(!is.null(model_pca)){
predict(object = model_pca, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA},
lasso = if(!is.null(model_lasso)){
predict_glmmLasso(census = test, m = model_lasso, L1.x = L1.x, lasso.L2.x = L2.x, L2.unit = L2.unit, L2.reg = L2.reg)
} else{NA},
gb = if(!is.null(model_gb)){
gbm::predict.gbm(object = model_gb, newdata = test, n.trees = model_gb$n.trees, type = "response")
} else{NA},
svm = if(!is.null(model_svm)){
as.numeric(attr(predict(object = model_svm, newdata = test, probability = TRUE),"probabilities")[,"1"])
} else{NA},
mrp = if(!is.null(model_mrp)){
predict(object = model_mrp, newdata = test, type = "response", allow.new.levels = TRUE)
} else{NA}
)
# remove NA's
test_preds <- test_preds[,apply(X = test_preds, MARGIN = 2, FUN = function(x){
all(!is.na(x))})]
# outcome on the test
# test_y <- dplyr::select(.data = test, one_of(y))
test_y <- dplyr::pull(.data = test, y)
# EBMA
if(verbose){
forecast.data <- EBMAforecast::makeForecastData(
.predCalibration = data.frame(train_preds),
.outcomeCalibration = train_y,
.predTest = data.frame(test_preds),
.outcomeTest = test_y)
forecast.out <- EBMAforecast::calibrateEnsemble(
forecast.data,
model = "normal",
useModelParams = FALSE,
tol = tol[idx.tol])
} else {
forecast.data <- quiet(
EBMAforecast::makeForecastData(
.predCalibration = data.frame(train_preds),
.outcomeCalibration = as.numeric(unlist(train_y)),
.predTest = data.frame(test_preds),
.outcomeTest = as.numeric(unlist(test_y)))
)
forecast.out <- quiet(EBMAforecast::calibrateEnsemble(
forecast.data,
model = "normal",
useModelParams = FALSE,
tol = tol[idx.tol]))
}
# mse
mse_collector[idx.Ndraws, idx.tol] <- mean(( as.numeric(unlist(test_y)) -
as.numeric( attributes(forecast.out)$predTest[,1,1]))^2)
# model weights
weights_box[idx.Ndraws, , idx.tol] <- attributes(forecast.out)$modelWeights
# progress
if (verbose) cat(paste("\n","EBMA: ", round(counter / (length(tol) * n.draws),2)*100, "% done",sep=""))
}
}
# which tolerance value minimizes the mse on the test set
best_tolerance <- apply(mse_collector, 1, function(x) which.min(x))
# container of best model weights
weights_mat <- matrix(data = NA, nrow = n.draws, ncol = ncol(train_preds))
# model weights; rows = observations, columns = model weights, layers = tolerance values
if (length(tol)>1){
for (idx.tol in 1:length(best_tolerance)){
weights_mat[idx.tol, ] <- weights_box[idx.tol, ,][ ,best_tolerance[idx.tol]]
}
} else{
weights_mat <- weights_box[, , 1]
}
# average model weights
if (is.null(dim(weights_mat))){
final_model_weights <- as.numeric(weights_mat)
} else{
final_model_weights <- apply(weights_mat, 2, mean)
}
names(final_model_weights) <- names(train_preds)
}
# weighted average
model_preds <- as.matrix(post.strat$predictions$Level2[,names(final_model_weights)])
model_preds
w_avg <- as.numeric(model_preds %*% final_model_weights)
# deal with missings
if (any(is.na(w_avg))){
# missing row indexes
m_idx <- which(is.na(w_avg))
# which classifiers contain missings
NA_classifiers <- which(apply(model_preds, 2, function(x) any(is.na(x))))
# readjust weights without that classifier
NA_adj_weights <- final_model_weights[-NA_classifiers] / sum(final_model_weights[-NA_classifiers])
# remove columns with NAs
no_Na_preds <- model_preds[
m_idx, # rows
apply(X = model_preds, MARGIN = 2, FUN = function(x){all(!is.na(x))}) # columns
]
# predictions for rows with NAs on at least 1 classifier
no_NA_avg <- as.numeric(no_Na_preds %*% NA_adj_weights)
# replace predictions
w_avg[m_idx] <- no_NA_avg
}
dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit)
L2.unit
# L2 preds object
L2_preds <- dplyr::tibble(
!!L2.unit = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
# L2 preds object
L2_preds <- dplyr::tibble(
rlang::!!L2.unit = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
# L2 preds object
L2_preds <- dplyr::tibble(
rlang::syms(L2.unit) = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
# L2 preds object
L2_preds <- dplyr::tibble(
!!rlang::syms(L2.unit) = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
# L2 preds object
L2_preds <- dplyr::tibble(
!! rlang::syms(L2.unit) = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
# L2 preds object
L2_preds <- dplyr::tibble(
rlang::syms(!!L2.unit) = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit)
# L2 preds object
L2_preds <- dplyr::tibble(
rlang::syms(!!L2.unit) = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
L2.unit
# L2 preds object
L2_preds <- dplyr::tibble(
!! L2.unit = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
# L2 preds object
L2_preds <- dplyr::tibble(
L2.unit = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
L2_preds
# L2 preds object
L2_preds <- dplyr::tibble(
!! rlang::sym(L2.unit) = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
?rlang::as_name()
?rlang::as_name(rlang::sym(L2.unit))
rlang::as_name(rlang::sym(L2.unit))
rlang::as_name(L2.unit)
rlang::as_name(L2.unit)
# L2 preds object
L2_preds <- dplyr::tibble(
!! rlang::as_name(L2.unit) = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
base::as.name(L2.unit)
# L2 preds object
L2_preds <- dplyr::tibble(
!! base::as.name(L2.unit) = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
)
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg
) %>%
dplyr::rename( !! base::as.name(L2.unit) = state )
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg) %>%
dplyr::rename( !! base::as.name(L2.unit) = state )
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg) %>%
dplyr::rename(L2.unit = state )
L2_preds
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg) %>%
dplyr::rename( !! L2.unit = state )
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg) %>%
dplyr::rename( rlang::sym(L2.unit) = state )
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg) %>%
dplyr::rename( !! rlang::sym(L2.unit) = state )
!! rlang::sym(L2.unit)
rlang::sym(L2.unit)
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg) %>%
dplyr::rename( !! rlang::as_name(L2.unit) = state )
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg) %>%
dplyr::rename( !! base::as.name(L2.unit) = state )
setNames
setNames(L2.unit)
L2_preds
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit)
dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit)
w_avg
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg) %>%
dplyr::rename( !! base::as.name(L2.unit) := state )
L2_preds
# L2 preds object
L2_preds <- dplyr::tibble(
state = dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg) %>%
dplyr::rename( !! rlang::sym(L2.unit) := state )
L2_preds
# L2 preds object
L2_preds <- dplyr::tibble(
!! rlang::sym(L2.unit) := dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg)
L2_preds
# L2 preds object
L2_preds <- dplyr::tibble(
!! rlang::sym(L2.unit) := dplyr::pull(.data = post.strat$predictions$Level2, var = L2.unit),
ebma = w_avg)
L2_preds
library(autoMrP)
